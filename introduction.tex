\chapter{Introduction}


\section{Error Analysis}

Constituency parser performance is primarily interpreted through a single
metric, F-score on \wsj section 23, that conveys no linguistic information
regarding the remaining errors.  We classify errors within a set of
linguistically meaningful types using tree transformations that repair groups
of errors together.  We use this analysis to answer a range of questions about
parser behaviour, including what linguistic constructions are difficult for
state-of-the-art parsers, what types of errors are being resolved by rerankers,
and what types are introduced when parsing out-of-domain text.

\section{Overview}

Parsing has been a major area of research within computational linguistics for
decades, and constituent parser F-scores on \wsj section 23 have exceeded
$90\%$ \cite{Petrov-Klein:2007}, and $92\%$ when using self-training and
reranking \cite{McClosky-Charniak-Johnson:2006,Charniak-Johnson:2005}. 
While
these results give a useful measure of overall performance, they provide no
information about the nature, or relative importance, of the remaining errors.

Broad investigations of parser errors beyond the \parseval metric
\cite{Black-etal:1991} have either focused on specific parsers, \eg
\cite{Collins:2003}, or have involved conversion to dependencies
\cite{Carroll-etal:1998,King:2003}.  In all of these cases, the analysis has
not taken into consideration how a set of errors can have a common cause, \eg a
single mis-attachment can create multiple node errors.

We propose a new method of error classification using tree transformations.
Errors in the parse tree are repaired using subtree movement, node
creation, and node deletion.  Each step in the process is then associated with
a linguistically meaningful error type, based on factors such as the node that is
moved, its siblings, and parents.  

Using our method we analyse the output of thirteen constituency parsers on
newswire.  Some of the frequent error types that we identify are widely recognised
as challenging, such as prepositional phrase (PP) attachment.  However, other
significant types have not received as much attention, such as clause
attachment and modifier attachment.

Our method also enables us to investigate where reranking and self-training
improve parsing.  Previously, these developments were analysed only in terms of
their impact on F-score.  Similarly, the challenge of out-of-domain parsing has
only been expressed in terms of this single objective.  We are able to
decompose the drop in performance and show that a disproportionate number of
the extra errors are due to coordination and clause attachment.

This work presents a comprehensive investigation of parser behaviour in terms
of linguistically meaningful errors.  By applying our method to multiple
parsers and domains we are able to answer questions about parser behaviour that
were previously only approachable through approximate measures, such as counts
of node errors.  We show which errors have been reduced over the past fifteen
years of parsing research; where rerankers are making their gains and where
they are not exploiting the full potential of k-best lists; and what types of
errors arise when moving out-of-domain.  We have released our
system\footnote{http://code.google.com/p/berkeley-parser-analyser/} to enable
future work to apply our methodology.

\section{Graph Parsing}

\subsection{Old abstract}
General treebank analyses are graph structured, but parsers are typically restricted to tree structures for efficiency and modeling reasons.
We propose a new representation and algorithm for a restricted class of graph structures that are flexible enough to cover almost all treebank structures, yet are restricted enough to admit efficient learning and inference.
In particular, we consider directed, acyclic, one-endpoint-crossing graph structures, which cover most dislocation, shared argumentation, and similar tree-violating linguistic phenomena.
We describe how to convert phrase structure parses, including traces, to our new representation, in a reversible manner.
Our dynamic program uniquely decomposes structures, is sound and complete with respect to a subset of the class of one-endpoint-crossing graphs, and covers $97.7\%$ of the Penn English treebank.
We also implement a proof-of-concept parser that recovers a range of null elements and trace types.

\subsection{Old intro}

Many syntactic representations use graphs and/or discontinuous structures, such as traces in Government and Binding theory, and f-structure in Lexical Functional Grammar \cite{gb,Bresnan:1982}.
Sentences in the Penn Treebank \cite{ptb} have a core projective tree structure, and trace edges that represent control structures, wh-movement and more.
However, most parsers and the standard evaluation metric ignore these edges and all null elements.
By leaving out parts of the structure, they fail to provide all predicates to downstream tasks such as question answering.
While there has been work on capturing some parts of this extra structure, it has generally either been through post-processing on trees
\cite{Johnson:2002,Jijkoun:2003,Campbell:2004,Levy:2004,Gabbard:2006},
or has only captured a limited set of phenomena via grammar augmentation
\cite{collins:1997,dienes-dubey:2003,schmid:2006,cai-chiang-goldberg:2011}.
%%%In both cases phenomena such as shared argumentation are completely ignored.
%%%Similarly, most work on the Abstract Meaning Representation \cite{amr}, has removed edges to turn all structures into trees.

\begin{figure}
  \centering
  \scalebox{0.65}{
  \input{figures/picture-simple-flat}
  }
  \vspace{-5mm}
  \caption{\label{fig:repr}
    Parse representation: (a) constituency (b) ours.
  }
\end{figure}

We propose a new parse representation and a new algorithm that can efficiently consider almost all syntactic phenomena.
Our representation is an extension of TAG-based tree representations \cite{cck,Shen:2007}, modified to represent graphs and designed to maximize coverage under a new class of graphs.
Our algorithm extends a non-projective tree parsing algorithm \cite{ec} to graph structures, with improvements to avoid derivational ambiguity.

Our representation, shown in Figure~\ref{fig:repr}b, consists of complex tags composed of non-terminals, and edges indicating attachment.
In this form, traces can create problematic structures such as directed cycles, but we show how careful choice of head rules can minimize such issues.

Our algorithm runs in time $O(n^4)$ under a first-order model.
We also introduce extensions that ensure parses contain a directed projective tree of non-trace edges.
We implemented a proof-of-concept parser with a basic first-order model, scoring $88.3$ on trees in section 22, and recovering a range of trace types.

Together, our representation and algorithm form an inference method that can cover $97.7\%$ of sentences, far above the coverage of projective tree algorithms ($46.8\%$).
