\begin{abstract}

Representation of syntactic structure is a core area of research in Computational Linguistics, disambiguating distinctions in meaning that are crucial for correct interpretation of language.
Development of algorithms and statistical models over the past three decades has led to systems that are accurate enough to be deployed in industry, playing a key role in products such as Google Search and Apple Siri.
However, syntactic parsers today are usually constrained to tree representations of language, and performance is interpreted through a single metric that conveys no linguistic information regarding remaining errors.

In this dissertation, we present new algorithms for error analysis and parsing.
The heart of our approach to error analysis is the use of structural transformations to identify more meaningful classes of errors, and to enable comparisons across formalisms.
For parsing, we combine a novel dynamic program with careful choices in syntactic representation to create an efficient parser that produces graph structured output.
In both cases, we are pushing the boundaries of the conventional parsing task, bringing it closer to the linguistic theories it is based on.

First, we present a search algorithm that, given two structures, finds a sequence of modifications leading from one structure to the other.
We applied this algorithm to syntactic error analysis, where one structure is the output of a parser, the other is the correct parse, and each modification corresponds to fixing one error.
We constructed a tool based on the algorithm and analyzed variations in behavior between parsers, types of text, and languages.
Our observations shine light on several assumptions about syntactic errors, showing some to be true and others to be false.
For example, prepositional phrase attachment errors are indeed a major issue, while coordination scope errors do not hurt performance as much as expected.

Next, we describe an algorithm that builds a parse in one syntactic representation based on a parse in another.
Specifically, we build phrase structure parses from Combinatory Categorial Grammar derivations.
Our approach follows the philosophy of \ccg, defining specific phrase structures for each lexical category and generic rules for combinatory steps.
The new parse is built by following the \ccg derivation bottom-up, gradually building the corresponding phrase structure parse.
This produces significantly more accurate parses than past work, and has the advantage that the process decomposes in the same way as the CKY parsing algorithm.
We use the tool to compare performance of parsers across formalisms % TODO

Finally, we present a dynamic programming algorithm, which constructs the graph structure that has the highest score under an edge-factored scoring function.
In the process of defining a parse representation compatible with the algorithm, we found that certain linguistic distinctions could dramatically impact coverage.
We also show various ways to modify the algorithm to improve performance by exploiting properties of observed linguistic structure.
This approach to syntactic parsing is the first to cover virtually all structure encoded in the Penn Treebank.


\end{abstract}
