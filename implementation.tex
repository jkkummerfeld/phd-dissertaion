\section{Implementation}

While the core algorithm is fully described in the previous sections, a range of further ideas are needed to enable efficient parsing and modern training methods.
Each section below describes one component of the system, its optimisations and the motivation behind them.

\section{Overall Architecture}

- overall structure

- every optimisation

\subsection{Learning}

We use a discriminative model, trained with loss-augmented decoding, using an update schedule defined by AdaGrad (TODO CITE), and lazy updates as described in (TODO CITE).


\subsubsection{Loss Function}

- Describe how we have to account for scores when doing loss-augmented decoding

\subsection{Inference}

\subsubsection{State beams}

In each cell of the chart we use a beam, discarding items based on their viterbi inside score.
We ensure diversity by dividing each beam into a collection of sub-beams.
In all three passes, the sub-beams separate items based on their type ($N$, $L$, etc), and the parents of each position in the item.
This definition enables us to avoid considering most incompatible items.
The third pass also includes one of the spines in the sub-beam definition for the same reason.

\subsubsection{Cube pruning}
\parencite{Chiang:2007}

We apply the standard cube pruning approach when doing binary and ternary compositions.
Since we are using sub-beams to determine which items are compatible, we use a heap of sub-cubes during composition.
Using fine sub-beams to avoid comparing incompatible items means that there are many of these sub-cubes, and so we also prune entire sub-cubes based on the score of their top item.

\subsubsection{Coarse to Fine Pruning}
\parencite{Goodman:1997}
Rather than parsing immediately with the full model we use three passes with progressively richer structure:
(1) projective trees without spines,
(2) non-projective graphs without spines,
(3) full structure.
Each pass prunes using max-marginals from the preceding pass.
The third pass also prunes spines that are not consistent with at least one unpruned edge from the second pass.

 - discussion of the trace pruning

 - discussion of the spine pruning

 - discussion of rule pruning

\subsubsection{Outside Pass Calculations}

There are two main classes of algorithms for parsing that our algorithm can be used within: Viterbi and Inside--Outside.
The first of these finds the optimal structure for a sentence under a given model and in the process determines the optimal substructure for every span of the sentence.
That's sufficient for parsing, 


When using our algorithm with a model that only places weights on the edges, computing max-marginals and sum-marginals is straightforward.
Each edge exists in only one place in the derivation, between the item without it and the item with it.

Once spines are introduced the situation changes because we would like to score them in all of the items they appear in.
This scoring is important for
% TODO

\subsubsection{Algorithm rule pruning}

- show all the rules, with prunable ones in blue

- discuss what we're pruning and what the impact is


\section{Results}

\subsection{Algorithm Coverage}

\begin{table}
  \centering
  \begin{tabular}{|lrr|}
    \hline
      & Acyclic & Has a cycle \\
    \hline
    \hline
%%%    Projective Tree & \textbf{18,637} & 0 \\
%%%    Projective Graph & \textbf{13,594} & 358 \\
%%%    One-Endpoint Crossing & \textbf{6,879} & 129 \\
%%%    Other Graph & 228  & 7 \\
    Projective Tree & \textbf{46.8\%} & - \\
    Projective Graph & \textbf{34.1\%} & 0.9\% \\
    One-Endpoint Crossing & \textbf{17.3\%} & 0.3\% \\
    Other Graph & 0.6\% & 0.01\% \\
    \hline
  \end{tabular}
  \caption{\label{tab:structures}
    Number of sentences in the training set that are of each structure type.
    Values in bold are for structures that are recoverable using our algorithm.
  }
\end{table}

\begin{table}
\centering
  \vspace{2mm}
  \begin{tabular}{|lrr|}
    \hline
%%%    & \multicolumn{2}{c}{Problematic (\%)} \\
    & \multicolumn{2}{c}{Coverage (\%)} \\
    Approach & Sentences & Edges \\
    \hline
    \hline
%%%    No traces & 56.15 & 3.73 \\
%%%    \hline
%%%    Core representation & 23.31 & 1.53 \\
%%%    + Head rule changes & 3.32 & 0.45 \\
%%%    + Null reversal & 2.28 & 0.41 \\
%%%    + Gapping shift & 1.81 & 0.38 \\
    Projective trees & 43.85 & 96.27 \\
    Core representation (3.1) & 76.69 & 98.47 \\
    + Head rule changes (3.3) & 96.68 & 99.55 \\
    + Null reversal (3.2) & 97.72 & 99.59 \\
    + Gapping shift (3.2) & 98.19 & 99.62 \\
    \hline
  \end{tabular}
  \caption{\label{tab:coverage}
    Coverage improvements for parts of our representation.
    Core uses the representation proposed in this paper, with the head rules from \textcite{cck}.
    Edge results are when removing only the edges necessary to make a parse representable (\myeg removing one edge to break a cycle).
  }
\end{table}

Table~\ref{tab:structures} divides sentences in the training set of the treebank by structure type and whether a directed cycle is present or not.
Structures that are recoverable using our algorithm are bolded.
The structures we consider cover almost all sentences, while projective trees, the standard output of parsers, account for less than half of sentences.

In Table~\ref{tab:coverage} we show the impact of design decisions for our representation.
The percentages indicate how many sentences in the training set are completely recoverable by our algorithm.
Each row shows the outcome of an addition to the previous row, starting from no traces at all, going to our representation with the head rules of \textcite{cck}, then changing the head rules, reversing null-null edges, and changing the target of edges in gapping.
The largest gain comes from changing the head rules, which is unsurprising since \textcite{cck}'s rules were designed for trees, where any set of rules produce valid structures.

\subsection{Problematic Structures}

To understand what structures are still not covered by our approach we manually inspected twenty examples that contained a cycle and twenty that were not one-endpoint-crossing.
For the cycles, eleven of the cases related to sentences containing variations of NP \emph{said} interposed between two parts of a single quote.
A cycle was present because the top node of the parse was co-indexed with a null argument of \emph{said} while \emph{said} was an argument of the head word of the quote.
The remaining cases were split between use of Expletive (5) and Interpret Constituent Here (4) traces.

%%% 3 EXP
%%% 5 ICH
%%% 2 NP says
%%% 1 Gapping
%%% 1 list
%%% 8 other
For the cases where the structure was not one-endpoint-crossing it was more difficult to determine trends.
The same three cases, ICH, EXP, and NP \emph{said} accounted for half of the issues.
Of the rest, most involved a set of arcs like the crossing arcs in Figure~\ref{fig:not-1ec}, but there was no clear way to avoid the crossings by adjusting head rules.

\subsection{Parsing Performance}

We implemented a proof-of-concept system to get preliminary results using this algorithm and representation.
We used only first-order features, \myie features do not consider pairs of edges.
Our code for both the algorithm and conversion to and from our representation will be made available.

First we considered the standard parsing metric for trees.
After one training pass through sections $2$--$21$ of the PTB on sentences up to length $40$, we get an F-score of $88.26$ on section $22$.
This is lower than other systems, including the \textcite{cck} parser, which scores $92.0$ on all sentences.
However, our result does show that even with simple features and limited training our algorithm can parse at a non-trivial level of accuracy.

For full graph parsing we considered sentences up to length $25$ (half of the treebank).
For unlabeled trace edges, we obtain a precision of $73\%$ and a recall of $15\%$.
Using \textcite{Johnson:2002}'s metric, which requires the label and span of the source and target nodes in the parse to be correct, we get precision $52\%$ and recall $18\%$.
This is lower than \textcite{Johnson:2002}'s results ($73$ and $63$ on all sentences).
However, we have shown that our algorithm can recover trace edges and expect that it can improve with feature development and longer training on the full corpus.

Finally, we measured the speed of the complete parser.
Pruning thresholds for the first two passes were tuned to retain $99\%$ of non-trace edges.
With that setting, the first pass prunes all but $0.43\%$ of possible edges, and $48.9\%$ of chart cells.
The second pass prunes all but $0.2\%$ of trace edges, and $64.3\%$ of chart cells.
However, at this threshold while $99\%$ of non-trace edges are retained, only $88.8\%$ of trace edges are retained.
With these settings, for sentences up to length $25$, it took $57$ seconds per sentence.

