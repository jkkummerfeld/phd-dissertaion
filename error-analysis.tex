\chapter{Automatic Error Analysis} \label{chp:analysis}

\begin{center}
\textit{
  Preliminary versions of this chapter appeared as \textcite{Kummerfeld-etal:2012:EMNLP} and \textcite{Kummerfeld-etal:2013:ACL}.
}
\end{center}

Constituency parser performance is primarily interpreted through a single metric, F-score on \wsj section 23, that conveys no linguistic information regarding the remaining errors.
In this chapter, we describe a new error analysis method that classifies errors within a set of linguistically meaningful types using tree transformations that repair groups of errors together.
We use this analysis to answer a range of questions about parser behavior, including what linguistic constructions are difficult for state-of-the-art parsers, what types of errors are being resolved by rerankers, what types are introduced when parsing out-of-domain text, and how the challenges change for Chinese.

%%% TODO:
%%% - Add 'treebank', 'parser' sideways labels to figures
%%%	Something else to think about - this paper is about 'what',
%%%	not 'why', so we don't use concepts like subcategorisation
%%%	Errors related to temporal structures would be a sensible
%%%	part though

\begin{figure}
\begin{center}
\scalebox{\derivscale}{
\synttree
[S
	[NP
		[PRP [He]]]
	[VP
		[VBD [was]]
		[VP
			[VBN [named]]
			[\extranode{S}
				[\extranode{NP}
					[NP [.t chief executive officer]]
					[\extranode{PP}
						[IN [of]]
						[\extranode{NP}
							[NP
								[NNP [Applied]]]
							[PP [.t in 1986]]]]]]]]]
}

(a) Parser output
\vspace{3mm}

\scalebox{\derivscale}{
\synttree
[S
	[NP
		[PRP [He]]]
	[VP
		[VBD [was]]
		[VP
			[VBN [named]]
			[\missingnode{S}
				[\missingnode{NP}
					[NP [.t chief executive officer]]
					[\missingnode{PP}
						[IN [of]]
						[NP
							[NNP [Applied]]]]]]
			[PP [.t in 1986]]]]]
}

(b) Gold parse
\end{center}
\derivspace
\caption[Error analysis example: PP Attachment.]{ \label{fig:PP-attachment}
	\textbf{Incorrect PP~Attachment}, as \emph{(PP in~1986)} is too low.
	Bold, boxed nodes are either extra (marked in red in the parser output) or missing (marked in blue in the gold parse).
}
\derivaftercompress
\end{figure}

\section{Error Classification} \label{sec:errors-intro}

The standard metric for parsing is:

\begin{align*}
  \text{F}_1 & = \frac{2 * \text{precision} * \text{recall}}{\text{precision} + \text{recall}} \\
            & = \frac{2 * |\text{matching nodes}|}{|\text{nodes in gold parse}| + |\text{nodes in predicted parse}|}
\end{align*}

This is a robust metric that gives a sense of overall parser performance, but a single number cannot provide linguistically meaningful intuition for the source of remaining errors.
Since the metric is defined in terms of nodes, one natural direction for analysis is to break down the errors by node type.
Unfortunately, that approach is not particularly informative, as a single attachment error can cause multiple node errors, without a clear link between the two once aggregated.
For example, in Figure~\ref{fig:PP-attachment} there is a PP attachment error that causes seven bracket errors (extra S, NP, PP, and NP, missing S, NP, and PP).
Determining that these correspond to a PP attachment error from just the labels of the missing and extra nodes is difficult.
Additionally, once an aggregate count of node errors is made, different types of errors may contribute to the same count, for example mistakes in PP attachment and co-ordination could both contribute to the count of incorrect NP nodes.
% TODO: elaborate further
In contrast, the approach we describe below takes into consideration the relations between node errors, grouping them into linguistically meaningful sets.

We classify node errors in two phases.
First, we find a set of tree transformations that convert the output parse into the gold parse (Section~\ref{sec:tree-transform}).
Second, each transformation is classified as one of several error types (Section~\ref{sec:classify}).
Pseudocode for our method is shown in Algorithm~\ref{alg:code}.
The tree transformation stage corresponds to the main loop, while the second stage corresponds to the final loop.

\begin{algorithm}[t]
\begin{algorithmic}
\State $U =$ initial set of node errors
\State Sort $U$ by the depth of the error in the parse, deepest first
\State $G = \emptyset$
\Repeat
	\ForAll{errors $e \in U$}
		\If{$e$ fits an environment template $t$}
			\State $g =$ new error group
			\State Correct $e$ as specified by $t$
			\ForAll{errors $f$ that $t$ corrects}
				\State Remove $f$ from $U$
				\State Insert $f$ into $g$
			\EndFor
			\State Add $g$ to $G$
		\EndIf
	\EndFor
\Until{unable to correct any further errors}
\ForAll{remaining errors $e \in U$}
	\State Insert a group into $G$ containing $e$
\EndFor
\ForAll{groups $g \in G$}
	\State Classify $g$ based on properties of the group
\EndFor
\end{algorithmic}
\caption{ \label{alg:code}
	Tree transformation error classification
}
\end{algorithm}

\subsection{Tree Transformations} \label{sec:tree-transform}

Our algorithm finds a path from the parser output to the gold parse, where states are parse structures and steps are transformations.
We define three general types of transformations:

\begin{description}
  \item[Create node] takes a set of nodes that are siblings and places them under a new node.
  \item[Delete node] removes a node and re-attaches its children where it was.
  \item[Move subtree] takes a node and moves it either up or down within the parse, with the constraint that it may not make the structure discontinuous. If this move leads to the creation of a unary production between two non-terminals of the same type, also remove one of them.
\end{description}

The first two of these are straightforward, while an example of the third can be seen in Figure~\ref{fig:PP-attachment}.
The PP node spanning \emph{in 1986} is moved up in the parse until it is outside of all the extra spans.
This fixes most of the errors, but leaves behind an NP to NP unary production.
One of the NPs is deleted, leaving behind the correct parse.

Once a transformation is performed, all of the nodes that were fixed are placed in a single group and information about the nearby parse structure before and after the transformation is recorded.
This recording is necessary, since subsequent changes may alter the surrounding structure in ways that impact the classification process described in the next section.

With this definition of states and transformations, a range of standard search algorithms are available to us.
We considered search algorithms with guarantees for finding the shortest path, but found they were prohibitively slow.
Instead, we find a path by applying a greedy bottom--up approach, iterating through the errors in the parse, deepest first.
Fortunately, on the shorter sentences for which finding the optimal path was feasible, we found that the greedy approach had no substantial impact on results.
This is the case because in many sentences a single correction will fix the parse and in cases with multiple errors they are often in disjoint sections of the parse.

\subsection{Transformation Classification} \label{sec:classify}

We began with a large set of node errors, in the first stage they were placed into groups, one group per tree transformation used to get from the automatically generated parse to the gold parse.
Now we classify each group as one of the error types below.
The ideal set of error classes would consider each group of incorrect constituents and show how their function has been misinterpreted by the parser, but it is difficult to define simple rules for such a classification.
In future work it would be interesting to build statistical models for classifying errors, using expert analysis of parse errors to train the system.
In the meantime, the approach we took was to construct general rules that classify errors mainly by the category of the nodes being moved and their surrounding context.
We developed our rules by running the system with no classification, then progressively adding rules to decrease the number of unclassified errors.
In the process of adding rules we also inspected observed classifications to check that they were correct, amending the rules if necessary.

\paragraph{PP Attachment} (Figure~\ref{fig:PP-attachment}) \\
Any case in which the transformation involved moving a Prepositional Phrase, or the incorrect bracket is over a PP, \myeg \\ 
\emph{He was} (VP \emph{named chief executive officer of} (NP \emph{Applied} (PP \emph{in 1986}))) \\
where (PP \emph{in 1986}) should modify the entire VP, rather than just \emph{Applied}.

\begin{figure}
\centering
\input{figures/error-coordination}
\end{figure}

\paragraph{Coordination} (Figure~\ref{fig:coordination}) \\
Cases in which a conjunction is an immediate sibling of the nodes being moved, or is the leftmost or rightmost node being moved, \myeg \\
(NP \emph{A 16\% drop} (PP \emph{for} (NP (NP \emph{Mannesmann AG}) \emph{and} (NP \emph{Dresdner AG's 10\% decline})))) \\
where the conjunction and second NP should be at the top level, between \emph{A 16\% drop for Mannesmann AG} and \emph{Dresdner AG's 10\% decline}.

\paragraph{NP Attachment} (Figure~\ref{fig:NP-attachment}) \\
Several cases in which NPs had to be moved, particularly for mistakes in appositive constructions and incorrect attachments within a verb phrase, \myeg \\
\emph{The bonds} (VP \emph{go} (PP \emph{on sale} (NP \emph{Oct.\@\xspace 19}))) \\
where \emph{Oct.\@\xspace 19} should be an argument of \emph{go}.

\paragraph{Clause Attachment} (Figure~\ref{fig:clause-attachment}) \\
Any group that involves movement of some form of S node, \myeg \\
\emph{intends} (S (VP \emph{to} (VP \emph{restrict the RTC to ...\@\xspace} (SBAR \emph{unless the agency ...\@\xspace})))) \\
where the SBAR should be modifying \emph{intends}, rather than the lower VP.

\begin{landscape}
\begin{figure}
\centering
\begin{minipage}[b]{3.5in}
\centering
\input{figures/error-NP-attachment}
\end{minipage}\hfill
\begin{minipage}[b]{4.5in}
\centering
\input{figures/error-clause-attachment}
\end{minipage}
\end{figure}
\end{landscape}

\begin{figure}
\centering
\input{figures/error-VP-attachment}
\end{figure}

\paragraph{VP Attachment} (Figure~\ref{fig:VP-attachment}) \\
When the correction involves moving a VP, \myeg \\
(NP (NP \emph{federal financing}) (PP \emph{of research}) (VP \emph{using fetal tissue})) \\
where the VP should modify \emph{research}, not \emph{financing}.

\paragraph{Adverb and Adjective Modifier Attachment} (Figure~\ref{fig:modifier-attachment}) \\
Cases involving incorrectly placed adjectives and adverbs, including errors corrected by subtree movement and errors requiring only creation of a node, \myeg \\
(NP (ADVP \emph{even more}) \emph{severe setbacks}) \\
where there should be an extra ADVP node over \emph{even more severe}.

\paragraph{Unary} (Figure~\ref{fig:unary}) \\
Mistakes involving unary productions that are not linked to a nearby error such as a matching extra or missing node, \myeg \\
(SINV (VP \emph{following}) \emph{is a breakdown of major market activity}) \\
where the VP should have an S above it.
In this particular example we can see how not modeling traces may be hurting performance--the S is intended to span both the VP and a null NP that is co-indexed with \emph{a breakdown of major market activity}.

\begin{landscape}
\begin{figure}
\centering
\begin{minipage}[b]{3in}
\input{figures/error-modifier-attachment}
\end{minipage}\hfill
\begin{minipage}[b]{5in}
\centering
\input{figures/error-unary}
\end{minipage}
\end{figure}
\end{landscape}

\begin{figure}
\hfill
\begin{minipage}[b]{2.5in}
\centering
\input{figures/error-different-label}
\end{minipage}\hfill
\begin{minipage}[b]{3.5in}
\centering
\input{figures/error-single-word-phrase}
\end{minipage}\hfill
\end{figure}

\paragraph{Different label} (Figure~\ref{fig:different-label}) \\
In many cases a node is present in the parse that spans the correct set of words, but has the wrong label, in which case we group the two node errors, (one extra, one missing), as a single error, \myeg \\
(PP \emph{Unlike} (ADVP \emph{two years ago})) \\
where the ADVP should be an NP.

\paragraph{Single word phrase} (Figure~\ref{fig:single-word-phrase}) \\
Node errors that span a single word, \myeg \\
(PP (ADVP \emph{Shortly}) \emph{after 10 a.m.\@\xspace}) \\
where the ADVP should not be present.
We include checks to ensure this is not linked to another error, such as one part of a set of internal noun phrase errors.

\paragraph{Parenthetical Attachment} (Figure~\ref{fig:PRN-attachment})
When a parenthetical node modifies at the wrong level, \myeg \\
(NP \emph{other forms} (PP \emph{of} (NP \emph{housing} (PRN \emph{(such as low-income)})))) \\
where the parenthetical should be higher.

\paragraph{Missing Parenthetical} (Figure~\ref{fig:missing-PRN})
When a parenthetical is entirely missing, \myeg \\
(S \emph{Moreover, explains one official, \ldots}) \\
where there should be a PRN around \emph{, explains one official,}.

\begin{landscape}
\begin{figure}
\begin{minipage}[b]{4in}
\centering
\input{figures/error-PRN-attachment}
\end{minipage}\hfill
\begin{minipage}[b]{4in}
\centering
\input{figures/error-missing-PRN}
\end{minipage}
\end{figure}
\end{landscape}

\begin{figure}
\centering
\input{figures/error-NP-internal-structure}
\end{figure}

\paragraph{NP Internal Structure} (Figure~\ref{fig:NP-internal-structure}) \\
While most NP structure is not annotated in the \ptb, there is some use of ADJP, NX, NAC and QP nodes.
We form a single group for each NP that has one or more errors involving these types of nodes, \myeg \\
(PP \emph{Within} (NP \emph{about six months})) \\
should have a QP over \emph{about six}.

\paragraph{Other}
There is a long tail of other errors.
Some could be placed within the categories above, but would require far more specific rules.

Working from only a raw list of node errors in a parse, it would be difficult to determine errors like those described above.
Even for error types that can be measured by counting node errors or rule production errors, our approach has the advantage that we identify groups of errors with a single cause.
For example, what appears as a missing unary production may correspond to an extra bracket that contains a subtree that attached incorrectly.
By grouping node errors and classifying each group as a single mistake, we are able to more precisely characterize the mistakes a parser makes.

\section{Results} \label{sec:errors-results}

We used sections 00 and 24 of the WSJ section of the \ptb as development data while constructing the tree transformation and error group classification methods.
All of our examples in text come from these sections as well, but for all tables of results we ran our system on section 23.
We chose to run our analysis on section 23 as it is the only section we are sure was not used in the development of any of the parsers, either for tuning or feature development.
Our evaluation is entirely focused on the errors of the parsers, so unless there is a particular construction that is unusually prevalent in section 23, we are not revealing any information about the test set that could bias future work.

Our evaluation is over a wide range of \ptb constituency parsers and their variants from the past twenty years.
For all parsers we used the publicly available version, with the standard parameter settings.

\begin{table}
\begin{center}
\begin{tabular}{|lccccr|}
	\hline
		System & P & R & F & Exact & Speed \\
	\hline
	\hline
		\multicolumn{6}{|c|}{Enhanced Training / Systems} \\
		Charniak S \& R (SR)       & 92.44 & 91.70 & 92.07 & 44.87 & 1.8 \\
		Charniak Re ranking (R)     & 91.78 & 91.04 & 91.41 & 44.04 & 1.8 \\
		Charniak Self-trained (S)  & 91.16 & 90.89 & 91.02 & 40.77 & 1.8 \\
	\hline
		\multicolumn{6}{|c|}{Standard Parsers} \\
		Berkeley                   & 90.30 & 89.81 & 90.06 & 36.59 & 4.2 \\
		Charniak                   & 89.88 & 89.55 & 89.71 & 37.25 & 1.8 \\
		SSN                        & 89.96 & 88.89 & 89.42 & 32.74 & 1.8 \\
		BUBS                       & 88.57 & 88.43 & 88.50 & 31.62 & 27.6 \\
		Bikel                      & 88.23 & 88.10 & 88.16 & 32.33 & 0.8 \\
		Collins-3                  & 87.82 & 87.50 & 87.66 & 32.22 & 2.0 \\
		Collins-2                  & 87.77 & 87.48 & 87.62 & 32.51 & 2.2 \\
		Collins-1                  & 87.29 & 86.90 & 87.09 & 30.35 & 3.3 \\
		Stanford Lexicalized (L)   & 86.35 & 86.49 & 86.42 & 27.65 & 0.7 \\
		Stanford Unlexicalized (U) & 86.48 & 85.09 & 85.78 & 28.35 & 2.7 \\
	\hline
\end{tabular}
\caption[\parseval results on \wsj section 23 for the parsers we consider.]{ \label{tab:standard-results} 
	\parseval results on \wsj section 23 for the parsers we consider.
  The columns are precision, recall, F-score, exact sentence match, and speed (sentences / sec).
  Coverage was left out as it was above 99.8\% for all parsers.
}
\end{center}
\end{table}

% TODO: Expand descriptions of parsers evaluated
\paragraph{Berkeley} \parencite{Petrov-etal:2006,Petrov-Klein:2007}.
An unlexicalised parser with a grammar constructed with automatic state splitting.

\paragraph{\textcite{Bikel:2004}} implementation of \textcite{Collins:1997}.

\paragraph{BUBS} \parencite{Dunlop-Bodenstab-Roark:2011,Bodenstab-Dunlop-Hall-Roark:2011}.
A `grammar-agnostic constituent parser,' which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy.

\paragraph{\textcite{Charniak:2000}}.
A generative parser with a maximum entropy-inspired model.
We also use the reranker \parencite{Charniak-Johnson:2005}, and the self-trained model \parencite{McClosky-Charniak-Johnson:2006}.

\paragraph{\textcite{Collins:1997}}.
A generative lexicalized parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes into account traces.

\paragraph{SSN} \parencite{Henderson:2003,Henderson:2004}.
A statistical left-corner parser, with probabilities estimated by a neural network.

\paragraph{Stanford} \parencite{Klein-Manning:2003:ACL,Klein-Manning:2003:NIPS}.
We consider both the unlexicalised PCFG parser (-U) and the lexicalized factored parser (-L), which combines the PCFG parser with a lexicalized dependency parser.

Table~\ref{tab:standard-results} shows the standard performance metrics,
measured on section 23 of the \wsj, using all sentences.  Speeds were measured
using a Quad-Core Xeon CPU (2.33GHz 4MB L2 cache) with 16GB of RAM.
These results clearly show the variation in parsing performance, but they do
not show which constructions are the source of those variations.

\begin{table}
\begin{center}
\begin{tabular}{|lrrr|}
	\hline
		           &             & Nodes    &       \\
		Error Type & Occurrences & Involved & Ratio \\
	\hline
	\hline
		PP Attachment & 846 & 1455 & 1.7 \\
		Single Word Phrase & 490 & 490 & 1.0 \\
		Clause Attachment & 385 & 913 & 2.4 \\
		Adverb and Adjective Modifier Attachment & 383 & 599 & 1.6 \\
		Different Label & 377 & 754 & 2.0 \\
		Unary & 347 & 349 & 1.0 \\
		NP Attachment & 321 & 597 & 1.9 \\
		NP Internal Structure & 299 & 352 & 1.2 \\
		Coordination & 209 & 557 & 2.7 \\
		Unary Clause Label & 185 & 200 & 1.1 \\
		VP Attachment & 64 & 159 & 2.5 \\
		Parenthetical Attachment & 31 & 74 & 2.4 \\
		Missing Parenthetical & 12 & 17 & 1.4 \\
		Unclassified & 655 & 734 & 1.1 \\
	\hline
\end{tabular}
\caption[Breakdown of errors on section 23 for the Charniak parser with self-trained model and reranker.]{ \label{tab:charniak-breakdown}
	Breakdown of errors on section 23 for the Charniak parser with self-trained model and reranker.
	Errors are sorted by the number of times they occur.
	Ratio is the average number of node errors caused by each error we identify
	(\myie Nodes Involved / Occurrences).
}
\end{center}
\end{table}

Our system enables us to answer questions about parser behavior that could previously only be probed indirectly.
We demonstrate its usefulness by applying it to a range of parsers (here), to reranked K-best lists of various lengths, and to output for out-of-domain parsing (following sections).

First, in Table~\ref{tab:charniak-breakdown}, we focus on a detailed error breakdown for the best system we consider, the Charniak parser with a self-trained model and with reranking of the top fifty candidate parses produced by the parser \parencite{Charniak:2000,Charniak-Johnson:2005,McClosky-Charniak-Johnson:2006}.
For each of our error types, the table shows how many times it occurred in the 2,416 sentences in \wsj section 23 of the \ptb (Occurrences), the total number of nodes involved in the groups that were classified as each error type (Nodes Involved), and the ratio between the two (Nodes / Occurrences).

The ratios show that some errors typically cause only a single node error, where as others, such as coordination, generally cause several.
This means that considering counts of error groups would over-emphasize some error types, \myeg single word phrase errors are second most important by number of groups, but seventh by total number of node errors (not counting `Unclassified', which corresponds to many different errors).

In Table~\ref{tab:wsj23-comp} we consider the breakdown of parser errors on \wsj section 23.
The shaded area of each bar indicates the frequency of parse errors (\myie empty means fewest errors).
The area filled in is determined by the average number of node errors per sentence that are attributed to that type of error.
The average number of node errors per sentence for a completely full bar is indicated by the bottom row (Worst), and the value for a completely empty bar is indicated by the top row (Best).
We use counts of node errors to make the contributions of each error type to F-score more interpretable.

As expected, PP attachment is the largest contributor to errors, across all parsers.
Coordination is surprisingly low on the list (6th) given how significant the problem is considered in the community \parencite{N06-1020}.
This appears to indicate that parsers are better at coordination than at PP attachment, but the raw counts do not take into consideration the relative frequency of the two decisions.
We can get a sense of how frequent these decisions are by counting CCs and PPs in sections 02--21 of the treebank: 16,844 and 95,581 respectively.
These counts are only an indicator of the number of decisions as the nodes can be used in ways that do not involve a decision.
Taking into consideration the 6:1 ratio of decisions, the 4:1 ratio of errors is less surprising, and instead supports the belief that coordination scope ambiguity is very difficult.
However, it does appear that improvements in coordination accuracy will have a limited impact on our performance metrics.
One surprisingly common error involves unary productions.
Looking at the breakdown by unary type we found that clause labeling (S, SINV, etc) accounted for a large proportion of the errors.

\begin{landscape}
% TODO: Make spacing consistent in columns of these figures
\input{figures/wsj23_comparison.tex}
\end{landscape}

By comparing the performance of the Collins parser with other systems we can see where performance has and has not changed over the past fifteen years.
There has been improvement across the board, but in some cases, \myeg clause attachment errors and different label errors, the change has been more limited (24\% and 29\% reductions respectively).
It is difficult to know why these ambiguities are more difficult, but three possibilities are:
(1) annotation errors may be more prevalent, making the potential for improvement vary,
(2) they pose fundamental difficulties for the community's overall approach to parsing,
(3) there may be biases in our automatic classification scheme.
We investigated the breakdown of the different label errors by label, but no particular cases of label confusion stand out, and we found that the most common cases remained the same between Collins and the top results.
Overall, it seems that these error types may have been unintentionally neglected in research and could be a productive area for future investigation.

It is also interesting to compare pairs of parsers that share aspects of their
architecture.  One such pair is the Stanford parser, where the factored parser
combines the unlexicalised parser with a lexicalized dependency parser.  The
main sources of the 0.64 gain in F-score are PP attachment and coordination.

Another interesting pair is the Berkeley parser and the BUBS parser, which uses
a Berkeley grammar, but improves speed by pruning.  The pruning methods used in
BUBS are particularly damaging for PP attachment errors and unary errors.

Various comparisons can be made between Charniak parser variants.  We discuss
the reranker below.  For the self-trained model
\textcite{McClosky-Charniak-Johnson:2006} performed some error analysis,
considering variations in F-score depending on the frequency of tags such as
PP, IN and CC in sentences.  Here we see gains on all error types, though
particularly for clause attachment, modifier attachment and coordination, which
fits with their observations.

\subsection{Reranking}

The standard dynamic programming approach to parsing limits the range of features that can be employed.
One way to deal with this issue is to modify the parser to produce the top $K$ parses, rather than just the 1-best, then use a model with more sophisticated features to choose the best parse from this list \parencite{collins:00}.
While re-ranking has led to gains in overall performance \parencite{Charniak-Johnson:2005}, there has been limited analysis of how effectively rerankers are using the list of parses they are ranking.
Recent work has explored this question in more depth, but focusing on how variation in inference and model parameters impacts performance on standard metrics \parencite{huang:08a,Ng-etal:2010,Auli-Lopez:2011,Ng-Curran:2012}.
We will explore the question from the perspective of observed errors, considering the potential for improvement using only the n-best list of parses, and how many errors a re-ranker actually avoids.

In Table~\ref{tab:reranking} we present a breakdown over error types for the
Charniak parser, using the self-trained model and reranker.  The oracle results
use the parse in each K-best list with the highest F-score.  While this may not
give the true oracle result, as F-score does not factor over sentences, it
gives a close approximation.  The table has the same columns as
Table~\ref{tab:wsj23-comp}, but the ranges on the bars now reflect the min and
max for these sets.

\begin{landscape}
\input{figures/reranking.tex}
\end{landscape}

While there is improvement on all errors when using the reranker, there is very
little additional gain beyond the first 5-10 parses.  Even for the oracle
results, most of the improvement occurs within the first 5-10 parses.  The
limited utility of extra parses for the reranker may be due to the importance
of the base parser output probability feature (which, by definition, decreases
within the K-best list).
Another possibility is that there is less useful variation further down the K-best list.
The utility is higher for the oracle, but we do not see greater improvement further down the list because those parses will be combinations of a set of variations in the parse that change the model probability only slightly, rather than providing useful variation.

Interestingly, the oracle performance considerably improves across all error types, even at the 2-best level.
This indicates that the base parser model is not particularly biased against particular error types, as if it were we would expect that fixing it would require going further down the list of parses given as options.
Focusing on the rows for $K=2$ we
can also see two interesting outliers.  The PP attachment improvement of the
oracle is considerably higher than that of the reranker, particularly compared
to the differences for other errors, suggesting that the reranker lacks the
features necessary to make the decision better than the parser.  The other
interesting outlier is NP internal structure, which continues to make
improvements for longer lists, unlike the other error types.

\subsection{Out-of-Domain}

Parsing performance drops considerably when shifting outside of the domain a
parser was trained on \parencite{Gildea:2001}.
Previously, \textcite{Clegg:2005:EIT:1626315.1626317} evaluated parsers qualitatively on
node types and rule productions, while  \textcite{Bender:2011:PEO:2145432.2145479}
designed a Wikipedia test set to evaluate parsers on dependencies representing
ten specific linguistic phenomena.

To provide a deeper understanding of the errors arising when parsing outside of
the newswire domain, we analyze performance of the Charniak parser with
reranker and self-trained model on the eight parts of the Brown corpus
\parencite{ptb}, and two parts of the Google Web
corpus \parencite{Petrov:2012}.  Table~\ref{tab:domain-info} shows statistics for
the corpora.  The variation in average sentence lengths skew the results for
errors per sentence.  To handle this we divide by the number of words to
determine the results in Table~\ref{tab:charniak-domains}, rather than by the
number of sentences, as in previous figures.

There are several interesting features in the table.  First, on the Brown
datasets, while the general trend is towards worse performance on all errors,
NP internal structure is a notable exception and in some cases PP attachment
and unaries are as well.

In the other errors we see similar patterns across the corpora, except humor
(Brown R), on which the parser is particularly bad at coordination and clause
attachment.  This makes sense, as the colloquial nature of the text includes
more unusual uses of conjunctions, for example here \emph{no mistake} is a phrase that is unlikely to be in the \wsj, and if it were, would probably be preceded by a verb like \emph{make}:

\vspace{3mm}
\emph{She was a living doll and no mistake -- the ... }
\vspace{3mm}

\begin{landscape}
\input{figures/domains_charniak.tex}
\end{landscape}

\begin{table}
\begin{center}
\begin{tabular}{|llrr|}
	\hline
		Corpus & Description & Sentences & Av. Length \\
	\hline
	\hline
		\wsj 23 & Newswire & 2416 & 23.5 \\
		Brown F & Popular & 3164 & 23.4 \\
		Brown G & Biographies & 3279 & 25.5 \\
		Brown K & General & 3881 & 17.2 \\
		Brown L & Mystery & 3714 & 15.7 \\
		Brown M & Science & 881 & 16.6 \\
		Brown N & Adventure & 4415 & 16.0 \\
		Brown P & Romance & 3942 & 17.4 \\
		Brown R & Humor & 967 & 22.7 \\
		G-Web Blogs & Blogs & 1016 & 23.6 \\
		G-Web Email & E-mail & 2450 & 11.9 \\
	\hline
\end{tabular}
\caption[Variation in size and contents of the domains we consider.]{ \label{tab:domain-info}
	Variation in size and contents of the domains we consider.	The variation in
	average sentence lengths skews the results for errors per sentences, and so
	in Table~\ref{tab:charniak-domains} we consider errors per word.
}
\end{center}
\end{table}

Comparing the Brown corpora and the Google Web corpora, there are much larger
divergences.  We see a particularly large decrease in NP internal structure.
Looking at some of the instances of this error, it appears to be largely caused
by incorrect handling of structures such as URLs and phone numbers, which do
not appear in the \ptb.  There are also some more difficult cases, for example:

\vspace{3mm}
\emph{... going up for sale in the next month or do .}
\vspace{3mm}

\noindent where \emph{or do} is a QP.
This typographical error of \emph{do} instead of \emph{two} is extremely difficult to handle for a parser trained only on well-formed text.

For e-mail there is a substantial drop on single word phrases.  Breaking the
errors down by label we found that the majority of the new errors are missing
or extra NPs over single words.  Here the main problem appears to be temporal
expressions, though there also appear to be a substantial number of errors that
are also at the POS level, such as when NNP is assigned to \emph{ta} in this
case:

\vspace{3mm}
\emph{... let you know that I 'm out ta here !}
\vspace{3mm}

Some of these issues, such as URL handling, could be resolved with suitable
training data.  Other issues, such as ungrammatical language and
unconventional use of words, pose a greater challenge.

%%%  ============================================================================
%%%  
%%%  This paper reports on the results of systematic analysis of errors
%%%  made by Chinese parsers.
%%%  
%%%  The experimental results are very interesting.
%%%  Though the basic methodology used in the paper builds directly
%%%  on a previous work, it is adapted well to Chinese through careful
%%%  analysis of error types.
%%%  
%%%  It would be nice to give more in-depth comparison with previous
%%%  findings on the difficulty of Chinese parsing, but I understand
%%%  the authors have already done well within the limited space.
%%%  
%%%  ============================================================================
%%%  
%%%  The paper is a thorough analysis of problems in Chinese parsing. I like it that
%%%  a variety of parsers were used. The paper is generally well written. The
%%%  results do not seem particularly surprising but it is good to have empirical
%%%  evidence to support them. The analysis and software should support future
%%%  research into the problematic areas.
%%%  However, I would suggest adding statistical significance tests, and reworking
%%%  Section 4. I understand that not all error categories can be discussed in
%%%  detail but the selection was unsatisfactory. Table 1 shows which error types
%%%  are new in this work, but not all of them are explained. It's not clear how the
%%%  order of subsections is determined. Some types are presented mostly to say that
%%%  they are less important than in English (PP attachment), while the second
%%%  biggest error type (coordination) is not discussed in this section at all, and
%%%  the third-highest only very briefly (not enough to make me understand why this
%%%  is substantially different from English).
%%%  Maybe for those categories that are comparable to Kummerfeld, you could add the
%%%  English percentages to Table 1?
%%%  I'm not so convinced by the final conclusion, NP internal structure is an issue
%%%  in English as well, just not in the PTB annotation. Coordination is still not
%%%  properly solved for English.
%%%  
%%%  ============================================================================
%%%  
%%%  This paper presents a comprehensive analysis of errors in Chinese Parsing,
%%%  and releases an open-source code for it.
%%%  
%%%  The author adapt the system from Kummerfeld et al.2012 to Chinese,
%%%  and examine a wide range of Chinese constitute Parsers.
%%%  Experimental results show a series of interesting findings,
%%%  mainly on verb-argument-attach-error and coordination errors.
%%%  The idea is clear and the paper is well written.
%%%  
%%%  What I miss is a more specific conclusion: I think it's not as deep as
%%%  Kummerfeld in 2012. Yes, empirically we can see the coord and verb args fields
%%%  are not quite improved even with gold POS tag. But I think it would be clearer
%%%  if the author can give some examples to explain why gold POS can't do that. And
%%%  this would help us to find the right direction to address these problems.

\section{Chinese Parsing}

Aspects of Chinese syntax result in a distinctive mix of parsing challenges.
However, the contribution of individual sources of error to overall difficulty is not well understood.  
We conduct a comprehensive automatic analysis of error types made by Chinese parsers, covering a broad range of error types for large sets of sentences, enabling the first empirical ranking of Chinese error types by their performance impact.  
To accommodate error classes that are absent in English, we augment the system to recognize Chinese-specific parse errors.
To understand the impact of part-of-speech tagging errors on different error types, we also performed a part-of-speech ablation experiment, in which particular confusions are introduced in isolation.
By analyzing the distribution of errors in the system output with and without gold part-of-speech tags, we are able to isolate and quantify the error types that can be resolved by improvements in POS tagging accuracy.
Our analysis shows that improvements in tagging accuracy can only address a subset of the challenges of Chinese syntax.
Further improvement in Chinese parsing performance will require research addressing other challenges, in particular, determining coordination scope.

\subsection{Background}

A decade of Chinese parsing research, enabled by the Penn Chinese Treebank \parencite[\pctb;][]{Xue:2005:NLE}, has seen Chinese parsing performance improve from 76.7 F$_1$ \parencite{Bikel-Chiang:2000:CLP} to 84.1 F$_1$ \parencite{Qian-Liu:2012:EMNLP}.
While recent advances have focused on understanding and reducing the errors that occur in segmentation and part-of-speech tagging \parencite{Qian-Liu:2012:EMNLP,Jiang-etal:2009:ACL,Forst-Fang:2009:EACL}, a range of substantial issues remain that are purely syntactic.

The closest previous work is the detailed manual analysis performed by \textcite{Levy-Manning:2003:ACL}.
While their focus was on issues faced by their factored PCFG parser \parencite{Klein-Manning:2003:NIPS}, the error types they identified are general issues presented by Chinese syntax in the \pctb.
They presented several Chinese error types that are rare or absent in English, including noun/verb ambiguity, NP-internal structure and coordination ambiguity due to \emph{pro}-drop.
They attributed some of these differences to treebank annotation decisions and others to meaningful differences in syntax, suggesting that closing the English-Chinese parsing gap demands techniques beyond those currently used for English.
Based on this analysis they considered how to modify their parser to capture the information necessary to model the syntax within the \pctb.
However, as noted in their final section, their manual analysis of parse errors in one hundred sentences only covered a portion of a single parser's output, limiting the conclusions they could reach regarding the distribution of errors in Chinese parsing.

\subsection{Adapting Error Analysis to Chinese} \label{sec:adapting_automatic_error_analysis_to_chinese}

Our analysis builds on the system described in Sections~\ref{sec:classify}~and~\ref{sec:tree-transform}, which finds the shortest path from the system output to the gold annotations, then classifies each transformation step into one of several error types.
When directly applied to Chinese parser output, the system placed over 27\% of the errors in the catch-all `Other' type.
Many of these errors clearly fall into one of a small set of error types, motivating an adaptation to Chinese syntax.

To adapt the system to Chinese, we developed a new version of the second stage of the system, which assigns an error category to each tree transformation step.

To characterize the errors the original system placed in the `Other' category,
we looked through one hundred sentences, identifying error types generated by
Chinese syntax that the existing system did not account for.
With these observations we were able to implement new rules to catch the
previously missed cases, leading to the set shown in Table~\ref{tab:errors}.
To ensure the accuracy of our classifications, we alternated between refining
the classification code and looking at affected classifications to identify
issues.  We also periodically changed the sentences from the development set
we manually checked, to avoid over-fitting.

Where necessary, we also expanded the information available during
classification.  For example, we use the structure of the final gold standard
parse when classifying errors that are a byproduct of sense disambiguation
errors.

\subsection{Chinese Parsing Errors} \label{sec:chinese_parsing_errors}

\begin{table}
\centering
\begin{tabular}{lrr}
  \hline
  Error Type & Brackets & \% of total \\
  \hline
  \hline
              NP-internal* & 6019 & 22.70\% \\
              Coordination & 2781 & 10.49\% \\
   Verb taking wrong args* & 2310 &  8.71\% \\
                     Unary & 2262 &  8.53\% \\
Adverb and Adjective Modifier Attachment & 1900 &  7.17\% \\
             One Word Span & 1560 &  5.88\% \\
           Different label & 1418 &  5.35\% \\
            Unary A-over-A & 1208 &  4.56\% \\
   Wrong sense/bad attachment* & 1018 &  3.84\% \\
      Noun boundary error* &  685 &  2.58\% \\
             VP Attachment &  626 &  2.36\% \\
         Clause Attachment &  542 &  2.04\% \\
             PP Attachment &  514 &  1.94\% \\
      Split Verb Compound* &  232 &  0.88\% \\
              Scope error* &  143 &  0.54\% \\
             NP Attachment &  109 &  0.41\% \\
                     Other & 3186 & 12.02\% \\
\hline
\end{tabular}
\caption[Breakdown of errors in Chinese parsing.]{ \label{tab:errors} 
  Errors made when parsing Chinese. Values are the number of bracket errors
  attributed to that error type. The values shown are for the Berkeley parser,
  evaluated on the development set. * indicates error types that were added or
  substantially changed as part of the adaptation to Chinese.
}
\end{table}

Table~\ref{tab:errors} presents the errors made by the Berkeley parser.
Below we describe the error types that are either new in this analysis, have had their definition altered, or have an interesting distribution.

In all of our results we follow the same approach as earlier, presenting the number of bracket errors (missing or extra) attributed to each error type.
As discussed in Section~\ref{sec:errors-results}, for the purpose of understanding the cause of the remaining performance gap, bracket counts are more informative than a direct count of each error type, because the impact on \parseval F-score varies between errors, \myeg a single attachment error can cause 20 bracket errors, while a unary error causes only one.

\begin{figure}
\centering
  \scalebox{1.2}{ \includegraphics{figures/chinese-tree2} }
%%%  \begin{tikzpicture}
%%%  \tikzset{every tree node/.style={align=center}}
%%%  \Tree
%%%  [.NP 
%%%    [.NN 国家\\national ]
%%%    [.NN 女足\\soccer ]
%%%    [.NN 教练\\coach ] ]
%%%  \end{tikzpicture}
  \caption[Error analysis example: NP internal (Chinese).]{ \label{fig:np_internal} 
    NP-internal structure errors, gold structure (left) and parser hypothesis (right).
    Note, unlike in Section~\ref{sec:classify}, the trees are displayed sideways here, so that the Chinese is closer to the traditional writing order (top to bottom).
  }
\end{figure}

\paragraph{NP-internal} (Figure~\ref{fig:np_internal}) \\
The \pctb annotates more NP-internal structure than the \ptb.
We assign this error type when a transformation involves words whose parts of speech in the gold parse are one of: CC, CD, DEG, ETC, JJ, NN, NR, NT and OD.

We investigated the errors that fall into the NP-internal category and found that 49\% of the errors involved the creation or deletion of a single pre-terminal phrasal bracket.
These errors arise when a parser proposes a parse in which POS tags (for instance, JJ or NN) occur as siblings of phrasal tags (such as NP), a configuration used by the PCTB bracketing guidelines to indicate complementation as opposed to adjunction \parencite{Xue:2005:NLE}.

\begin{figure}
\centering
  \scalebox{1.2}{ \includegraphics{figures/chinese-treebush} }
  \caption[Error analysis example: Verb taking wrong arguments (Chinese).]{ \label{fig:wrong_arg}
    Verb taking wrong arguments, gold structure (left) and parser hypothesis (right).
  }
\end{figure}

\paragraph{Verb taking wrong args} (Figure~\ref{fig:wrong_arg}) \\
This error type
arises when a verb \mbox{(\myeg~\glos{扭转}{reverse})} is hypothesized to take
an incorrect argument (\mbox{\glos{布什}{Bush}} instead of
\mbox{\glos{地位}{position}}).  Note that this also covers some of the errors
that were classified as NP Attachment for English, changing
the distribution for that type.

\paragraph{Unary}
For mis-application of unary rules we separate out instances in which the two brackets in the production have the the same label (A-over-A).
This case is created when traces are eliminated, a standard step in evaluation.
More than a third of unary errors made by the Berkeley parser are of the A-over-A type.
This can be attributed to two factors: (i) the \pctb annotates non-local dependencies using traces, and (ii) Chinese syntax generates more traces than English syntax, such as \emph{pro}-drop, the omission of arguments where the referent is recoverable from discourse \parencite{Guo-Wang-VanGenabith:2007:EMNLP}.
However, for parsers that do not return traces they are a benign error.

\begin{figure}
\centering
  \scalebox{1.2}{ \includegraphics{figures/chinese-treemodatt} }
  \caption[Error analysis example: adverb and adjective modifier attachment (Chinese).]{ \label{fig:mod_att}
    Modifier attachment ambiguity, gold structure (left) and parser hypothesis (right).
  }
\end{figure}

\paragraph{Adverb and Adjective Modifier attachment} (Figure~\ref{fig:mod_att}) \\
Incorrect modifier scope caused by modifier phrase attachment level.
This is less frequent in Chinese than in English: while English VP modifiers occur in pre- and post-verbal positions, Chinese only allows pre-verbal modification.

\begin{figure}
\centering
  \scalebox{1.2}{ \includegraphics{figures/chinese-tree1} }
  \caption[Error analysis example: Word sense confusion (Chinese).]{ \label{fig:sense} 
    Sense confusion, gold structure (left) and parser hypothesis (right).
  }
\end{figure}

\paragraph{Wrong sense/bad attachment} (Figure~\ref{fig:sense}) \\
This applies when the head word of a phrase receives the wrong POS, leading to an attachment error.
This error type is common in Chinese because of POS fluidity, \myeg the well-known Chinese verb/noun ambiguity often causes mis-attachments that are classified as this error type.

In Figure~\ref{fig:sense}, the word \mbox{\glos{投资}{invest}} has both noun and
verb senses. While the gold standard interpretation is the relative clause
\mbox{\Trans{firms that Macau invests in}}, the parser returned an NP
interpretation \mbox{\Trans{Macau investment firms}}.

\paragraph{Noun boundary error}  In this error type, a span is moved to a
position where the POS tags of its new siblings all belong to the
list of NP-internal structure tags which we identified above, reflecting
the inclusion of additional material into an NP.

\paragraph{Split verb compound} The \pctb annotations recognize several
Chinese verb compounding strategies, such as the serial verb construction
(\mbox{\glos{规划建设}{plan [and] build}}) and the resultative construction
(\mbox{\glos{煮熟}{cook [until] done}}), which join a bare verb to another
lexical item.  We introduce an error type specific to Chinese, in which such
verb compounds are split, with the two halves of the compound placed in
different phrases.

\paragraph{Scope error}
These are cases in which a new span must be added to more closely bind a modifier phrase (ADVP, ADJP, and PP), for example, in 
(IP (ADVP \mbox{\glos{仅}{Only}}) (NP \mbox{\glos{去年}{last year,}}) (NP \mbox{\glos{中国 银行}{Bank of China}}) \ldots), the ADVP should modify the first NP only, which can be indicated by adding an NP to form (NP (ADVP \mbox{\glos{仅}{Only}}) (NP \mbox{\glos{去年}{last year}})).

\paragraph{PP attachment} 
This error type is rare in Chinese, as adjunct PPs are pre-verbal.  It does
occur near coordinated VPs, where ambiguity arises about which of the conjuncts
the PP has scope over.  Whether this particular case is PP attachment or
coordination is debatable; we follow the approach above and
label it PP attachment.

\subsection{Chinese-English Comparison} \label{subsec:chinese_english_comparison}

It is difficult to directly compare error analysis results for Chinese and
English parsing because of substantial changes in the classification method,
and differences in treebank annotations.

As described in the previous section, the set of error categories considered for Chinese is very different to the set of categories for English.  
Even for some of the categories that were not substantially changed, errors may be classified differently because of cross-over between two categories (\myeg between Verb taking wrong args and NP Attachment).

\begin{landscape}
\begin{table*}
\centering
\setlength\fboxsep{0mm}
\setlength\fboxrule{0.05mm}
\begin{tabular}{|lccccccccccccc|}
	\hline
	    & NP &  & Verb &  & Mod. & 1-Word & Diff & Wrong & Noun & VP & Clause & PP & \\
	System \hfill F$_1$ & Int. & Coord & Args & Unary & Attach & Span & Label & Sense & Edge & Attach & Attach & Attach & Other \\
	\hline
	\hline
	\emph{Best}    & 1.54 & 1.25 & 1.01 & 0.76 & 0.72 & 0.21 & 0.30 & 0.05 & 0.21 & 0.26 & 0.22 & 0.18 & 1.87 \\
	Berk-G$\;$ \hfill 86.8 &  \mybar{3.117775} &  \mybar{5.719997} &  \mybar{4.868075} &  \mybar{4.092204} &  \mybar{3.414187} &  \mybar{1.560478} &  \mybar{2.304226} &  \mybar{0.417061} &  \mybar{3.003337} &  \mybar{4.124765} &  \mybar{4.199998} &  \mybar{3.250795} &  \mybar{3.627305} \\
	Berk-2 \hfill 81.8 &  \mybar{5.106060} &  \mybar{5.726296} &  \mybar{4.657457} &  \mybar{5.602480} &  \mybar{3.889412} &  \mybar{5.874318} &  \mybar{5.465338} &  \mybar{4.259078} &  \mybar{4.180596} &  \mybar{4.645864} &  \mybar{3.974996} &  \mybar{4.055024} &  \mybar{4.120634} \\
	Berk-1 \hfill 81.1 &  \mybar{5.637080} &  \mybar{5.854733} &  \mybar{4.914895} &  \mybar{5.637384} &  \mybar{4.160972} &  \mybar{5.409644} &  \mybar{5.105299} &  \mybar{4.288569} &  \mybar{4.581930} &  \mybar{4.594485} &  \mybar{4.516671} &  \mybar{4.351321} &  \mybar{4.375378} \\
	ZPAR \hfill 78.1 &  \mybar{5.603375} &  \mybar{6.374733} &  \mybar{5.487246} &  \mybar{7.386910} &  \mybar{5.851629} &  \mybar{5.863920} &  \mybar{6.833466} &  \mybar{4.423375} &  \mybar{5.725748} &  \mybar{5.299080} &  \mybar{6.791682} &  \mybar{6.653960} &  \mybar{5.833838} \\
	Bikel \hfill 76.1 &  \mybar{6.464990} &  \mybar{7.357862} &  \mybar{6.227640} &  \mybar{6.372585} &  \mybar{6.280885} &  \mybar{6.602543} &  \mybar{5.609351} &  \mybar{6.904680} &  \mybar{8.000000} &  \mybar{5.790822} &  \mybar{6.700012} &  \mybar{6.484650} &  \mybar{5.983631} \\
	Stan-F \hfill 76.0 &  \mybar{6.824621} &  \mybar{8.000000} &  \mybar{6.340422} &  \mybar{6.721463} &  \mybar{6.291805} &  \mybar{6.949312} &  \mybar{6.977479} &  \mybar{4.827803} &  \mybar{5.886282} &  \mybar{7.060545} &  \mybar{7.166674} &  \mybar{6.933336} &  \mybar{5.501970} \\
	Stan-U \hfill 70.0 &  \mybar{8.000000} &  \mybar{7.035766} &  \mybar{8.000000} &  \mybar{8.000000} &  \mybar{8.000000} &  \mybar{8.000000} &  \mybar{8.000000} &  \mybar{8.000000} &  \mybar{7.464874} &  \mybar{8.000000} &  \mybar{8.000000} &  \mybar{8.000000} &  \mybar{8.000000} \\
	\emph{Worst}   & 3.94 & 1.75 & 1.73 & 1.48 & 1.68 & 1.06 & 1.02 & 0.88 & 0.55 & 0.50 & 0.44 & 0.44 & 4.11 \\
	\hline
\end{tabular}
\caption[Error breakdown for a range of parsers on the \pctb.]{ \label{tab:comparison}
  Error breakdown for the development set of \pctb 6.  The area filled in for
  each bar indicates the average number of bracket errors per sentence attributed
  to that error type, where an empty bar is no errors and a full bar has
  the value indicated in the bottom row.  The parsers are:
  the Berkeley parser with gold POS tags as input (Berk-G),
  the Berkeley parser \parencite[Berk-1;][]{Petrov-etal:2006,Petrov-Klein:2007},
  the Berkeley product parser with two grammars \parencite[Berk-2;][]{Petrov:2010:NAACLHLT},
  ZPAR \textcite{Zhang-Clark:2009:ICPT},
  the Bikel parser \parencite{Bikel-Chiang:2000:CLP},
  the Stanford Factored parser \parencite[Stan-F;][]{Levy-Manning:2003:ACL,Klein-Manning:2003:NIPS},
  and the Stanford Unlexicalized PCFG parser \parencite[Stan-U;][]{Klein-Manning:2003:ACL}.
}
\end{table*}
\end{landscape}

Differences in treebank annotations also present a challenge for cross-language error comparison.
The most common error type in Chinese, NP-internal structure, is rare in the English results, but the datasets are not comparable because the \ptb has much more limited NP-internal structure annotated than the \pctb.
Further characterization of the impact of annotation differences on errors is beyond the scope of this work.

Three conclusions that can be made are that (i) coordination is a more common issue in Chinese, but remains difficult in
both languages, (ii) PP attachment is a much greater problem in English, and
(iii) substantial challenges are posed by the higher frequency of
syntactic structures generating traces and null-elements in Chinese compared to English.

\subsection{Cross-Parser Analysis} \label{sec:cross_parser_analysis}

The previous section described the error types and their distribution for a single Chinese parser.
Here we confirm that these are general trends, by showing that the same pattern
is observed for several different parsers on the \pctb 6 dev set.\footnote{
  We use the standard data split suggested by the \pctb 6 file manifest.
  As a result, our results differ from those previously reported on other splits.
}
We include results for
a transition-based parser \parencite[ZPAR;][]{Zhang-Clark:2009:ICPT},
a split-merge PCFG parser \parencite{Petrov-etal:2006,Petrov-Klein:2007,Petrov:2010:NAACLHLT},
a lexicalized parser \parencite{Bikel-Chiang:2000:CLP},
and a factored PCFG and dependency parser
\parencite{Levy-Manning:2003:ACL,Klein-Manning:2003:ACL,Klein-Manning:2003:NIPS}.
\footnote{These parsers represent a variety of parsing methods, though exclude
some recently developed parsers that are not publicly
available \parencite{Qian-Liu:2012:EMNLP,Xiong-etal:2005:IJCNLP}.}

Comparing the two Stanford parsers in Table~\ref{tab:comparison}, the factored
model provides clear improvements on sense disambiguation, but performs
slightly worse on coordination.

The Berkeley product parser we include uses only two grammars because we found,
in contrast to the English results \parencite{Petrov:2010:NAACLHLT}, that further
grammars provided limited benefits.  Comparing the performance with the
standard Berkeley parser it seems that the diversity in the grammars only
assists certain error types, with most of the improvement occurring in four of
the categories, while there is no improvement, or a slight decrease, in five categories.

\subsection{Tagging Error Impact} \label{sec:pos_ablation_study}

The challenge of accurate POS tagging in Chinese has been a major part of
several recent papers
\parencite{Qian-Liu:2012:EMNLP,Jiang-etal:2009:ACL,Forst-Fang:2009:EACL}.  The
Berk-G row of Table~\ref{tab:comparison} shows the performance of the Berkeley
parser when given gold POS tags.\footnote{We used the Berkeley parser as it was
the best of the parsers we considered.  Note that the Berkeley parser
occasionally prunes all of the parses that use the gold POS tags,
and so returns the best available alternative.  This leads to a POS accuracy of
99.35\%, which is still well above the parser's standard POS accuracy of
93.66\%.}
While the F$_1$ improvement is unsurprising, for the
first time we can clearly show that the gains are only in a subset of the error
types.  In particular, tagging improvement will not help for two of the most
significant challenges: coordination scope errors, and verb argument
selection.

To see which tagging confusions contribute to which error reductions, we adapt the POS
ablation approach of \textcite{Tse-Curran:2012:NAACL-HLT}.  We consider the POS
tag pairs shown in Table~\ref{tab:pos-confusion}.  To isolate the effects of
each confusion we start from the gold tags and introduce the output of the
Stanford tagger whenever it returns one of the two tags being
considered.\footnote{We introduce errors to gold tags, rather than removing
errors from automatic tags, isolating the effect of a single confusion
by eliminating interaction between tagging decisions.}
We then feed these ``semi-gold'' tags to the
Berkeley parser, and run the fine-grained error analysis on its output.

\begin{table}
  \centering
  \begin{tabular}{|llrr|}
    \hline
      \multicolumn{2}{c}{Confused tags} & Errors & $\Delta$ F$_1$ \\
    \hline
    \hline
      VV  & NN  & 1055 & -2.72 \\
      DEC & DEG &  526 & -1.72 \\
      JJ  & NN  &  297 & -0.57 \\
      NR  & NN  &  320 & -0.05 \\
    \hline
  \end{tabular}
  \caption[The most frequently confused POS tag pairs in Chinese parsing.]{ \label{tab:pos-confusion}
    The most frequently confused POS tag pairs.
    Each $\Delta$ F$_1$ is relative to Berk-G.
  }
\end{table}

\paragraph{VV/NN}  This confusion has been consistently shown to be a major
contributor to parsing errors
\parencite{Levy-Manning:2003:ACL,Tse-Curran:2012:NAACL-HLT,Qian-Liu:2012:EMNLP},
and we find a drop of over 2.7 $F_1$ when the output of the tagger is
introduced.  We found that while most error types have contributions from a
range of POS confusions, verb/noun confusion was responsible for virtually all of
the noun boundary errors corrected by using gold tags.

\paragraph{DEG/DEC}  This confusion between the relativizer and subordinator
senses of the particle \glos{的}{de} is the primary
source of improvements on modifier attachment when using gold tags.

\paragraph{NR/NN and JJ/NN}  Despite their frequency, these confusions have
little effect on parsing performance.  Even within the NP-internal error type
their impact is limited, and almost all of the errors do not change the
logical form.

\section{Summary}

The single F-score objective over brackets or dependencies obscures important differences between statistical parsers.
For instance, one or many mismatched brackets could be caused by a single attachment error.

In Sections~\ref{sec:tree-transform}~and~\ref{sec:classify}, we presented a novel tree-transformation methodology for
evaluating parsers that categorizes errors into linguistically meaningful
types.  Using this approach, we presented the first detailed examination of the
errors produced by a wide range of constituency parsers for
English and Chinese.  We found that PP attachment and clause attachment are the most
challenging constructions in English, while coordination turns out to be less problematic
than previously thought.  We also noted interesting variations in error types
for parser variants.

We investigated the errors resolved in reranking, and introduced by changing
domains. We found that the Charniak rerankers improved most error types, but
made little headway on improving PP attachment.  Changing domain has an impact
on all error types, except NP internal structure.

We also quantified the relative impacts of a comprehensive set of error types
in Chinese parsing.  Our analysis has shown that while improvements in
Chinese POS tagging can make a substantial difference for some error types,
it will not address two high-frequency error types: incorrect verb argument
attachment and coordination scope.  The frequency of these two error types is
also unimproved by the use of products of latent variable grammars.  These
observations suggest that resolving the core challenges of Chinese parsing
will require new developments that suit the distinctive properties of Chinese
syntax.

We released our system so that future constituent parsers could be evaluated using our methodology (see Appendix~\ref{chp:resources}).
Our analysis provides new insight into the development of parsers over the past fifteen years, and the challenges that remain.

