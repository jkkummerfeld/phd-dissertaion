\chapter{Automatic Error Analysis}

%%% TODO:
%%% - Add 'treebank', 'parser' sideways labels to figures
%%%	Something else to think about - this paper is about 'what',
%%%	not 'why', so we don't use concepts like subcategorisation
%%%	Errors related to temporal structures would be a sensible
%%%	part though

Previously published as "Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output", Jonathan K. Kummerfeld, David Hall, James R. Curran and Dan Klein.

\section{Parsers}

Our evaluation is over a wide range of \ptb constituency parsers and their variants from the past fifteen years.
For all parsers we used the publicly available version, with the standard parameter settings.

\begin{description}\itemsep1pt
	\item[ Berkeley] \cite{Petrov-etal:2006,Petrov-Klein:2007}. An
	unlexicalised parser with a grammar constructed with automatic state
	splitting.

	\item[ Bikel] (2004)\nocite{Bikel:2004} implementation of Collins (1997).

	\item[ BUBS]
	\cite{Dunlop-Bodenstab-Roark:2011,Bodenstab-Dunlop-Hall-Roark:2011}. A
	`grammar-agnostic constituent parser,' which uses a Berkeley Parser grammar,
	but parses with various pruning techniques to improve speed, at the cost of
	accuracy.

	\item[ Charniak] (2000)\nocite{Charniak:2000}. A generative parser with a
	maximum entropy-inspired model.  We also use the reranker
	\cite{Charniak-Johnson:2005}, and the self-trained model
	\cite{McClosky-Charniak-Johnson:2006}.

	\item[ Collins] (1997)\nocite{Collins:1997}. A generative lexicalised parser, with
	three models, a base model, a model that uses subcategorisation
	frames for head words, and a model that takes into account traces.

	\item[ SSN] \cite{Henderson:2003,Henderson:2004}. A statistical left-corner
	parser, with probabilities estimated by a neural network.

	\item[ Stanford] \cite{Klein-Manning:2003,Klein-Manning:2003:NIPS}. We
	consider both the unlexicalised PCFG parser (-U) and the factored parser
	(-F), which combines the PCFG parser with a lexicalised dependency parser.
\end{description}

\begin{table}
%%%\small
%%%\renewcommand{\tabcolsep}{1.7mm}
\begin{center}
\begin{tabular}{|lccccr|}
	\hline
		System & F & P & R & Exact & Speed \\
	\hline
	\hline
		\multicolumn{6}{|c|}{enhanced training / systems} \\
		Charniak-SR & 92.07 & 92.44 & 91.70 & 44.87 & 1.8 \\
		Charniak-R & 91.41 & 91.78 & 91.04 & 44.04 & 1.8 \\
		Charniak-S & 91.02 & 91.16 & 90.89 & 40.77 & 1.8 \\
	\hline
		\multicolumn{6}{|c|}{standard parsers} \\
		Berkeley & 90.06 & 90.30 & 89.81 & 36.59 & 4.2 \\
		Charniak & 89.71 & 89.88 & 89.55 & 37.25 & 1.8 \\
		SSN & 89.42 & 89.96 & 88.89 & 32.74 & 1.8 \\
		BUBS & 88.50 & 88.57 & 88.43 & 31.62 & 27.6 \\
		Bikel & 88.16 & 88.23 & 88.10 & 32.33 & 0.8 \\
		Collins-3 & 87.66 & 87.82 & 87.50 & 32.22 & 2.0 \\
		Collins-2 & 87.62 & 87.77 & 87.48 & 32.51 & 2.2 \\
		Collins-1 & 87.09 & 87.29 & 86.90 & 30.35 & 3.3 \\
		Stanford-L & 86.42 & 86.35 & 86.49 & 27.65 & 0.7 \\
		Stanford-U & 85.78 & 86.48 & 85.09 & 28.35 & 2.7 \\
	\hline
\end{tabular}
\caption{
	\label{tab:standard-results} \parseval results on \wsj section 23 for
	the parsers we consider.  The columns are F-score, precision, recall,
	exact sentence match, and speed (sents/sec).  Coverage was left out
	as it was above 99.8\% for all parsers.  In the {enhanced training /
	systems} section we include the Charniak parser with reranking (R), with a
	self-trained model (S), and both (SR).
}
\end{center}
%%%\vspace{-3mm}
\end{table}

Table~\ref{tab:standard-results} shows the standard performance metrics,
measured on section 23 of the \wsj, using all sentences.  Speeds were measured
using a Quad-Core Xeon CPU (2.33GHz 4MB L2 cache) with 16GB of RAM.
These results clearly show the variation in parsing performance, but they do
not show which constructions are the source of those variations.

\begin{figure}
\begin{center}
\scalebox{\derivscale}{
\synttree
[S
	[NP
		[PRP [He]]]
	[VP
		[VBD [was]]
		[VP
			[VBN [named]]
			[\wrongnode{S}
				[\wrongnode{NP}
					[NP [.t chief executive officer]]
					[\wrongnode{PP}
						[IN [of]]
						[\wrongnode{NP}
							[NP
								[NNP [Applied]]]
							[PP [.t in 1986]]]]]]]]]
}

\small
(a) Parser output
\vspace{3mm}

\scalebox{\derivscale}{
\synttree
[S
	[NP
		[PRP [He]]]
	[VP
		[VBD [was]]
		[VP
			[VBN [named]]
			[\wrongnode{S}
				[\wrongnode{NP}
					[NP [.t chief executive officer]]
					[\wrongnode{PP}
						[IN [of]]
						[NP
							[NNP [Applied]]]]]]
			[PP [.t in 1986]]]]]
}

\small
(b) Gold tree
\end{center}
\derivspace
\caption{
	\label{fig:PP-attachment}
	Grouping errors by node type is of limited usefulness.  In this figure and
	those that follow the top tree is the incorrect parse and the bottom tree is
	the correct parse.  Bold, boxed nodes are either extra (marked in the
	incorrect tree) or missing (marked in the correct tree).  This is an example
	of \textbf{PP~Attachment} (\emph{in~1986} is too low), but that is not at all
	clear from the set of incorrect nodes (extra S, NP, PP, and NP, missing S,
	NP, and PP).
}
\derivaftercompress
\end{figure}

\section{Error Classification}

While the statistics in Table~\ref{tab:standard-results} give a sense of
overall parser performance they do not provide linguistically meaningful
intuition for the source of remaining errors.  Breaking down the remaining
errors by node type is not particularly informative, as a single attachment
error can cause multiple node errors, many of which are for unrelated node
types.  For example, in Figure~\ref{fig:PP-attachment} there is a PP attachment
error that causes seven bracket errors (extra S, NP, PP, and NP, missing S,
NP, and PP).  Determining that these correspond to a PP attachment error from
just the labels of the missing and extra nodes is difficult.  In contrast, the
approach we describe below takes into consideration the relations between
errors, grouping them into linguistically meaningful sets.

We classify node errors in two phases.  First, we find a set of tree
transformations that convert the output tree into the gold tree.  Second, the
transformation are classified into error types such as PP attachment and
coordination.  Pseudocode for our method is shown in Algorithm~\ref{alg:code}.
The tree transformation stage corresponds to the main loop, while the second
stage corresponds to the final loop.

\begin{algorithm}[t]
\small
\begin{algorithmic}
\small
\State $U =$ initial set of node errors
\State Sort $U$ by the depth of the error in the tree, deepest first
\State $G = \emptyset$
\Repeat
	\ForAll{errors $e \in U$}
		\If{$e$ fits an environment template $t$}
			\State $g =$ new error group
			\State Correct $e$ as specified by $t$
			\ForAll{errors $f$ that $t$ corrects}
				\State Remove $f$ from $U$
				\State Insert $f$ into $g$
			\EndFor
			\State Add $g$ to $G$
		\EndIf
	\EndFor
\Until{unable to correct any further errors}
\ForAll{remaining errors $e \in U$}
	\State Insert a group into $G$ containing $e$
\EndFor
\ForAll{groups $g \in G$}
	\State Classify $g$ based on properties of the group
\EndFor
\end{algorithmic}
\caption{\small
\label{alg:code}
	Tree transformation error classification
}
\end{algorithm}

\subsection{Tree Transformation}

The core of our transformation process is a set of operations that move
subtrees, create nodes, and delete nodes.  Searching for the shortest path to
transform one tree into another is prohibitively slow.\footnote{We implemented
various search procedures and found similar results on the sentences that could
be processed in a reasonable amount of time.}  We find a path by applying a
greedy bottom--up approach, iterating through the errors in order of tree depth.
%%%in the tree, deepest first.

We match each error with a template based on nearby tree structure and
errors.  For example, in Figure~\ref{fig:PP-attachment} there are four extra
nodes that all cover spans ending at \emph{Applied in 1986}: S, NP, PP, NP.
There are also three missing nodes with spans ending between \emph{Applied} and
\emph{in}: PP, NP, and S.  Figure~\ref{fig:template} depicts these errors as
spans, showing that this case fits three criteria: (1) there are a set of extra
spans all ending at the same point, (2) there are a set of missing spans all
ending at the same point, and (3) the extra spans cross the missing spans,
extending beyond their end-point.  This indicates that the node starting after
\emph{Applied} is attaching too low and should be moved up, outside all of the
extra nodes.  Together, the criteria and transformation form a template.

\begin{figure}
\begin{center}
%%%\begin{pspicture}(-0.07,-0.6)(7.9,0.6)
%%%\psline(1.02,0.16)(1.02,0.75)(7.85,0.75)(7.85,0.16)
%%%\psline(4.52,0.16)(4.52,0.65)(7.75,0.65)(7.75,0.16)
%%%\psline(5.03,0.16)(5.03,0.55)(7.65,0.55)(7.65,0.16)
%%%\psline[linestyle=dashed,dash=4pt 2pt](1.02,0.02)(1.02,-0.51)(6.35,-0.51)(6.35,0.02)
%%%\psline[linestyle=dashed,dash=4pt 2pt](4.52,0.02)(4.52,-0.41)(6.25,-0.41)(6.25,0.02)
%%%\small named$\;$ chief$\;$ executive$\;$ officer$\;$ of$\;$ Applied$\;$ in$\;$ 1986
%%%\end{pspicture}
\end{center}
\vspace{-5mm}
\caption{
	\label{fig:template}
	Templates are defined in terms of extra and missing spans, shown here with
	unbroken lines above and dashed lines below, respectively.  This is an
	example of a set of extra spans that cross a set of missing spans (which in
	both cases all end at the same position).  If the last two words are moved,
	two of the extra spans will match the two missing spans.  The other extra
	span is deleted during the move as it creates an NP$\rightarrow$NP unary
	production.
}
\vspace{-3.5mm}
\end{figure}

Once a suitable template is identified we correct the error by moving subtrees,
adding nodes and removing nodes.  In the example this is done by moving the
node spanning \emph{in 1986} up in the tree until it is outside of all the
extra spans.  Since moving the PP leaves a unary production from an NP to an
NP, we also collapse that level.  In total this corrects seven errors, as there
are three cases in which an extra node is present that matches a missing node
once the PP is moved.  All of these errors are placed in a single group and
information about the nearby tree structure before and after the transformation
is recorded.

We continue to make passes through the list until no errors are corrected on a
pass.  For each remaining node error an individual error group is created.

The templates were constructed by hand based on manual analysis of parser
output.  They cover a range of combinations of extra and missing spans, with
further variation for whether crossing is occurring and if so whether the
crossing bracket starts or ends in the middle of the correct bracket.
Errors that do not match any of our templates are left uncorrected.

\subsection{Transformation Classification}

We began with a large set of node errors, in the first stage they were placed
into groups, one group per tree transformation used to get from the test tree
to the gold tree.  Next we classify each group as one of the error types below.

\begin{description}\itemsep1pt
	\item[ PP Attachment] Any case in which the transformation involved moving
	a Prepositional Phrase, or the incorrect bracket is over a PP, \eg \\ 
	\emph{He was} (VP \emph{named chief executive officer of} \\
	\textcolor{white}{fill}(NP \emph{Applied} (PP \emph{in 1986}))) \\
	where (PP \emph{in 1986}) should modify the entire VP, rather than just \emph{Applied}.

	\item[ NP Attachment] Several cases in which NPs had to be moved,
	particularly for mistakes in appositive constructions and incorrect
	attachments within a verb phrase, \eg \\ \emph{The bonds} (VP \emph{go} (PP
	\emph{on} \emph{sale} (NP \emph{Oct.\@\xspace 19}))) \\
	where \emph{Oct.\@\xspace 19} should be an argument of \emph{go}.
\end{description}


\begin{figure}
\begin{center}
\scalebox{\derivscale}{
\synttree
[VP
	[VBD [wrote]]
		[\wrongnode{NP}
			[NP
				[DT [another]]
				[JJ [new]]
				[NN [ad]]]
			[VBG [appearing]]]
		[NP
			[NN [today]]]]
}

\small
(a) Parser output

\vspace{3mm}

\scalebox{\derivscale}{
\synttree
[VP
	[VBD [wrote]]
		[\wrongnode{NP}
			[NP
				[DT [another]]
				[JJ [new]]
				[NN [ad]]]
			[\wrongnode{VP}
				[VBG [appearing]]
				[NP
					[NN [today]]]]]]
}

\small
(b) Gold tree
\end{center}
\derivspace
\caption{
	\label{fig:NP-attachment}
	\textbf{NP Attachment}: \emph{today} is too high, it should be the argument
	of \emph{appearing}, rather than \emph{wrote}.  This causes three node errors
	(extra NP, missing NP and VP).
}
\derivaftercompress
\end{figure}

\begin{figure}
\begin{center}
\scalebox{\derivscale}{
\synttree
[VP
	[VBD [had]]
	[\wrongnode{S}
		[\wrongnode{VP}
			[TO [to]]
			[\wrongnode{VP}
				[VB [think]]
				[PP [.t about it]]]]]
	[ADVP [.t ahead of time]]]
}

\small
(a) Parser output

\vspace{3mm}

\scalebox{\derivscale}{
\synttree
[VP
	[VBD [had]]
	[\wrongnode{S}
		[\wrongnode{VP}
			[TO [to]]
			[\wrongnode{VP}
				[VB [think]]
				[PP [.t about it]]
				[ADVP [.t ahead of time]]]]]]
}

\small
(b) Gold tree
\end{center}
\derivspace
\caption{
	\label{fig:modifier-attachment}
	\textbf{Modifier Attachment}: \emph{ahead of time} is too high, it should
	modify \emph{think}, not \emph{had}.  This causes six node errors (extra
	S, VP, and VP, missing S, VP, and VP).
}
\derivaftercompress
\end{figure}

\begin{figure}
\begin{center}
\scalebox{\derivscale}{
\synttree
[VP
	[VBZ [intends]]
	[\wrongnode{S}
		[\wrongnode{VP}
			[TO [to]]
			[\wrongnode{VP}
				[VB [restrict]]
				[NP [.t the RTC to \ldots]]
				[SBAR [.t unless the agency \ldots]]]]]]
}

\small
(a) Parser output

\vspace{3mm}

\scalebox{\derivscale}{
\synttree
[VP
	[VBZ [intends]]
	[\wrongnode{S}
		[\wrongnode{VP}
			[TO [to]]
			[\wrongnode{VP}
				[VB [restrict]]
				[NP [.t the RTC to \ldots]]]]]
	[SBAR [.t unless the agency \ldots]]]
}

\small
(b) Gold tree
\end{center}
\derivspace
\caption{
	\label{fig:clause-attachment}
	\textbf{Clause Attachment}: \emph{unless the agency receives specific
	congressional authorization} is attaching too low. This causes six
	node errors (extra S, VP, and VP, missing S, VP and VP).
}
\derivaftercompress
\end{figure}

\begin{figure}
\begin{center}
\scalebox{\derivscale}{
\synttree
[SINV
	[VP
		[VBG [Following]]]
	[VBZ [is]]
	[NP
		[NP [.t a breakdown]]
		[PP [.t of major market activity]]]]
}

\small
(a) Parser output

\vspace{3mm}

\scalebox{\derivscale}{
\synttree
[SINV
	[\wrongnode{S}
		[VP
			[VBG [Following]]]]
	[VBZ [is]]
	[\wrongnode{NP}
		[NP
			[NP [.t a breakdown]]
			[PP [.t of major market activity]]]]]
}

\small
(b) Gold tree

\vspace{3mm}

\scalebox{\derivscale}{
\synttree
[SINV
	[S-ADV
		[NP-SBJ
			[-NONE- [*-1]] ]
		[VP
			[VBG [Following]]]]
	[VBZ [is]]
	[NP-SBJ-1
		[NP
			[NP [.t a breakdown]]
			[PP [.t of major market activity]]]]
	[$\colon$ [$\colon$]]]
}

\small
(c) Gold tree with traces and function tags
\end{center}
\derivspace
\caption{
	\label{fig:unary}
	Two \textbf{Unary} errors, a missing S and a missing NP.  The third tree is
	the \ptb tree before traces and function tags are removed.  Note that the
	missing NP is over another NP, a production that does occur widely in the
	treebank, particularly over the word \emph{it}.
}
\derivaftercompress
\end{figure}


\begin{description}\itemsep1pt

	\item[ Modifier Attachment] Cases involving incorrectly placed adjectives
	and adverbs, including errors corrected by subtree movement and errors
	requiring only creation of a node, \eg \\ (NP (ADVP \emph{even more})
	\emph{severe setbacks}) \\ where there should be an extra ADVP node over
	\emph{even more severe}.
	\item[ Clause Attachment] Any group that involves movement of some form of
	S node.

	\item[ Unary] Mistakes involving unary productions that are not linked to
	a nearby error  such as a matching extra or missing node.  We do not include
	a breakdown by unary type, though we did find that clause labeling (S, SINV,
	etc) accounted for a large proportion of the errors.

	\item[ Coordination] Cases in which a conjunction is an immediate sibling
	of the nodes being moved, or is the leftmost or rightmost node being moved.

	\item[ NP Internal Structure]  While most NP structure is not annotated in
	the \ptb, there is some use of ADJP, NX, NAC and QP nodes.  We form a single
	group for each NP that has one or more errors involving these types of nodes.

	\item[ Different label] In many cases a node is present in the tree that
	spans the correct set of words, but has the wrong label, in which case we
	group the two node errors, (one extra, one missing), as a single error.

	\item[ Single word phrase] A range of node errors that span a single word,
	with checks to ensure this is not linked to another error (\eg one part of a
	set of internal noun phrase errors).

	\item[ Other] There is a long tail of other errors.  Some could be placed
	within the categories above, but would require far more specific rules.
\end{description}

\begin{figure}
\begin{center}
\scalebox{\derivscale}{
\synttree
[NP
	[NP [.t A 16\% drop]]
	[\wrongnode{PP}
		[IN [for]]
		[\wrongnode{NP}
			[NP [.t Mannesmann AG]]
			[CC [and]]
			[NP [.t Dresdner AG's 10\% decline]]]]]
}

\small
(a) Parser output

\vspace{3mm}

\scalebox{\derivscale}{
\synttree
[NP
	[\wrongnode{NP}
		[NP [.t A 16\% drop]]
		[\wrongnode{PP}
			[IN [for]]
			[NP [.t Mannesmann AG]]]]
	[CC [and]]
	[NP [.t Dresdner AG's 10\% decline]]]
}

\small
(b) Gold tree
\end{center}
\derivspace
\caption{
	\label{fig:coordination}
	\textbf{Coordination}: \emph{and Dresdner AG's 10\% decline} is too low.
	This causes four node errors (extra PP and NP, missing NP and PP).
}
\derivaftercompress
\end{figure}

\begin{figure}
\begin{center}
\scalebox{\derivscale}{
\synttree
[NP
	[NNP [Secretary]]
	[\wrongnode{PP}
		[IN [of]]
		[\wrongnode{NP}
			[NNP [State]]
			[NNP [Baker]]]]]
}

\small
(a) Parser output

\vspace{3mm}

\scalebox{\derivscale}{
\synttree
[NP
	[NNP [Secretary]]
	[\wrongnode{PP}
		[IN [of]]
		[\wrongnode{NP}
			[NNP [State]]]]
	[NNP [Baker]]]
}

\small
(b) Gold tree
\end{center}
\derivspace
\caption{
	\label{fig:NP-internal-structure}
	\textbf{NP Internal Structure}: \emph{Baker} is too low, causing four
	errors (extra PP and NP, missing PP and NP).
}
\derivaftercompress
\end{figure}

For many of these error types it would be difficult to extract a meaningful
understanding from only the list of node errors involved.  Even for error types
that can be measured by counting node errors or rule production errors, our
approach has the advantage that we identify groups of errors with a single
cause.  For example, a missing unary production may correspond to an extra
bracket that contains a subtree that attached incorrectly.

%%%\input{figures/wsj23_comparison.tex}

\subsection{Methodology}

We used sections 00 and 24 as development data while constructing the tree
transformation and error group classification methods.  All of our examples in
text come from these sections as well, but for all tables of results we ran our
system on section 23.  We chose to run our analysis on section 23 as it is the
only section we are sure was not used in the development of any of the parsers,
either for tuning or feature development.  Our evaluation is entirely focused
on the errors of the parsers, so unless there is a particular construction that
is unusually prevalent in section 23, we are not revealing any information
about the test set that could bias future work.

\section{Results}

\begin{table}
\small
\renewcommand{\tabcolsep}{1.7mm}
\begin{tabular}{|lrrr|}
	\hline
		                        &             & Nodes    &       \\
		Error Type\hspace{-8mm} & Occurrences & Involved & Ratio \\
	\hline
	\hline
		PP Attachment\hspace{-8mm} & 846 & 1455 & 1.7 \\
		Single word phrase\hspace{-8mm} & 490 & 490 & 1.0 \\
		Clause Attachment\hspace{-8mm} & 385 & 913 & 2.4 \\
		Modifier Attachment\hspace{-8mm} & 383 & 599 & 1.6 \\
		Different Label\hspace{-8mm} & 377 & 754 & 2.0 \\
		Unary & 347 & 349 & 1.0 \\
		NP Attachment\hspace{-8mm} & 321 & 597 & 1.9 \\
		NP Internal Structure\hspace{-8mm} & 299 & 352 & 1.2 \\
		Coordination & 209 & 557 & 2.7 \\
		Unary Clause Label\hspace{-8mm} & 185 & 200 & 1.1 \\
		VP Attachment & 64 & 159 & 2.5 \\
		Parenthetical Attachment\hspace{-8mm} & 31 & 74 & 2.4 \\
		Missing Parenthetical\hspace{-8mm} & 12 & 17 & 1.4 \\
		Unclassified & 655 & 734 & 1.1 \\
	\hline
\end{tabular}
\vspace{-2mm}
\caption{\label{tab:charniak-breakdown}
	Breakdown of errors on section 23 for the Charniak parser with self-trained
	model and reranker.  Errors are sorted by the number of times they occur.
	Ratio is the average number of node errors caused by each error we identify
	(\ie Nodes Involved / Occurrences).
}
\vspace{-6mm}
\end{table}

Our system enables us to answer questions about parser behaviour that could
previously only be probed indirectly.  We demonstrate its usefulness by
applying it to a range of parsers (here), to reranked K-best lists of various
lengths, and to output for out-of-domain parsing (following sections).

In Table~\ref{tab:wsj23-comp} we consider the breakdown of parser errors on
\wsj section 23.  The shaded area of each bar indicates the frequency of parse
errors (\ie empty means fewest errors).  The area filled in is determined by
the expected number of node errors per sentence that are attributed to that
type of error.  The average number of node errors per sentence for a completely
full bar is indicated by the Worst row, and the value for a completely empty
bar is indicated by the Best row. Exact error counts are available at
http://code.google.com/p/berkeley-parser-analyser/.

We use counts of node errors to make the contributions of each type of error
more interpretable.  As Table~\ref{tab:charniak-breakdown} shows, some errors
typically cause only a single node error, where as others, such as
coordination, generally cause several.  This means that considering counts of
error groups would over-emphasise some error types, \eg single word phrase
errors are second most important by number of groups (in
Table~\ref{tab:charniak-breakdown}), but seventh by total number of node errors
(in Table~\ref{tab:wsj23-comp}).

%%%\input{figures/reranking.tex}

As expected, PP attachment is the largest contributor to errors, across all
parsers.  Interestingly, coordination is sixth on the list, though that is
partly due to the fact that there are fewer coordination decisions to be made
in the treebank.\footnote{This is indicated by the frequency of CCs and PPs in
sections 02--21 of the treebank, 16,844 and 95,581 respectively.  These counts
are only an indicator of the number of decisions as the nodes can be used in
ways that do not involve a decision, such as sentences that start with a
conjunction.}

By looking at the performance of the Collins parser we can see the development
over the past fifteen years.  There has been improvement across
the board, but in some cases, \eg clause attachment errors and different label
errors, the change has been more limited (24\% and 29\% reductions respectively).  We
investigated the breakdown of the different label errors by label, but no
particular cases of label confusion stand out, and we found that the most
common cases remained the same between Collins and the top results.

It is also interesting to compare pairs of parsers that share aspects of their
architecture.  One such pair is the Stanford parser, where the factored parser
combines the unlexicalised parser with a lexicalised dependency parser.  The
main sources of the 0.64 gain in F-score are PP attachment and coordination.

Another interesting pair is the Berkeley parser and the BUBS parser, which uses
a Berkeley grammar, but improves speed by pruning.  The pruning methods used in
BUBS are particularly damaging for PP attachment errors and unary errors.
%%%Performing experiments varying the parameters of their pruning method is beyond
%%%the scope of this paper, but if these experiments were run our system could be
%%%applied directly to the output to determine which parameters are causing this
%%%behaviour.

Various comparisons can be made between Charniak parser variants.  We discuss
the reranker below.  For the self-trained model
\cite{McClosky-Charniak-Johnson:2006} performed some error analysis,
considering variations in F-score depending on the frequency of tags such as
PP, IN and CC in sentences.  Here we see gains on all error types, though
particularly for clause attachment, modifier attachment and coordination, which
fits with their observations.

\subsection{Reranking}

%%%\input{figures/domains_charniak.tex}

The standard dynamic programming approach to parsing limits the range of
features that can be employed.  One way to deal with this issue is to modify
the parser to produce the top $K$ parses, rather than just the 1-best, then use
a model with more sophisticated features to choose the best parse from this
list \cite{collins:00}.  While re-ranking has led to gains in performance
\cite{Charniak-Johnson:2005}, there has been limited analysis of how
effectively rerankers are using the set of available options.  Recent work has
explored this question in more depth, but focusing on how variation in the
parameters impacts performance on standard metrics
\cite{huang:08a,Ng-etal:2010,Auli-Lopez:2011,Ng-Curran:2012}.

In Table~\ref{tab:reranking} we present a breakdown over error types for the
Charniak parser, using the self-trained model and reranker.  The oracle results
use the parse in each K-best list with the highest F-score.  While this may not
give the true oracle result, as F-score does not factor over sentences, it
gives a close approximation.  The table has the same columns as
Table~\ref{tab:wsj23-comp}, but the ranges on the bars now reflect the min and
max for these sets.

While there is improvement on all errors when using the reranker, there is very
little additional gain beyond the first 5-10 parses.  Even for the oracle
results, most of the improvement occurs within the first 5-10 parses.  The
limited utility of extra parses for the reranker may be due to the importance
of the base parser output probability feature (which, by definition, decreases
within the K-best list).
%%%Another possibility is that there is less useful variation further down the K-best list.
%%%The utility is higher for the oracle, but
%%%we do not see greater improvement further down the list because 
%%%those parses
%%%will be combinations of a set of variations in the parse that change the
%%%model probability only slightly, rather than providing useful variation.

Interestingly, the oracle performance improves across all error types, even at
the 2-best level.  This indicates that the base parser model is not
particularly biased against a single error.  Focusing on the rows for $K=2$ we
can also see two interesting outliers.  The PP attachment improvement of the
oracle is considerably higher than that of the reranker, particularly compared
to the differences for other errors, suggesting that the reranker lacks the
features necessary to make the decision better than the parser.  The other
interesting outlier is NP internal structure, which continues to make
improvements for longer lists, unlike the other error types.

\subsection{Out-of-Domain}

Parsing performance drops considerably when shifting outside of the domain a
parser was trained on \cite{Gildea:2001}.
\cite{Clegg:2005:EIT:1626315.1626317} evaluated parsers qualitatively on
node types and rule productions.  \cite{Bender:2011:PEO:2145432.2145479}
designed a Wikipedia test set to evaluate parsers on dependencies representing
ten specific linguistic phenomena.

To provide a deeper understanding of the errors arising when parsing outside of
the newswire domain, we analyse performance of the Charniak parser with
reranker and self-trained model on the eight parts of the Brown corpus
\cite{Marcus-Marcinkiewicz-Santorini:1993}, and two parts of the Google Web
corpus \cite{Petrov:2012}.  Table~\ref{tab:domain-info} shows statistics for
the corpora.  The variation in average sentence lengths skew the results for
errors per sentence.  To handle this we divide by the number of words to
determine the results in Table~\ref{tab:charniak-domains}, rather than by the
number of sentences, as in previous figures.

\begin{table}
\small
\renewcommand{\tabcolsep}{1.6mm}
\begin{tabular}{|llrr|}
	\hline
		Corpus & Description & Sentences & Av. Length \\
	\hline
	\hline
		\wsj 23 & Newswire & 2416 & 23.5 \\
		Brown F & Popular & 3164 & 23.4 \\
		Brown G & Biographies & 3279 & 25.5 \\
		Brown K & General & 3881 & 17.2 \\
		Brown L & Mystery & 3714 & 15.7 \\
		Brown M & Science & 881 & 16.6 \\
		Brown N & Adventure & 4415 & 16.0 \\
		Brown P & Romance & 3942 & 17.4 \\
		Brown R & Humour & 967 & 22.7 \\
		G-Web Blogs & Blogs & 1016 & 23.6 \\
		G-Web Email & E-mail & 2450 & 11.9 \\
	\hline
\end{tabular}
\vspace{-2mm}
\caption{
	\label{tab:domain-info}
	Variation in size and contents of the domains we consider.	The variation in
	average sentence lengths skews the results for errors per sentences, and so
	in Table~\ref{tab:charniak-domains} we consider errors per word.
}
\vspace{-3mm}
\end{table}

There are several interesting features in the table.  First, on the Brown
datasets, while the general trend is towards worse performance on all errors,
NP internal structure is a notable exception and in some cases PP attachment
and unaries are as well.

In the other errors we see similar patterns across the corpora, except humour
(Brown R), on which the parser is particularly bad at coordination and clause
attachment.  This makes sense, as the colloquial nature of the text includes
more unusual uses of conjunctions, for example:

\vspace{3mm}
\emph{She was a living doll and no mistake -- the ... }
\vspace{3mm}

Comparing the Brown corpora and the Google Web corpora, there are much larger
divergences.  We see a particularly large decrease in NP internal structure.
Looking at some of the instances of this error, it appears to be largely caused
by incorrect handling of structures such as URLs and phone numbers, which do
not appear in the \ptb.  There are also some more difficult cases, for example:

\vspace{3mm}
\emph{... going up for sale in the next month or do .}
\vspace{3mm}

\noindent where \emph{or do} is a QP.  This typographical error is extremely
difficult to handle for a parser trained only on well-formed text.

For e-mail there is a substantial drop on single word phrases.  Breaking the
errors down by label we found that the majority of the new errors are missing
or extra NPs over single words.  Here the main problem appears to be temporal
expressions, though there also appear to be a substantial number of errors that
are also at the POS level, such as when NNP is assigned to \emph{ta} in this
case:

\vspace{3mm}
\emph{... let you know that I 'm out ta here !}
\vspace{3mm}

Some of these issues, such as URL handling, could be resolved with suitable
training data.  Other issues, such as ungrammatical language and
unconventional use of words, pose a greater challenge.

\section{Conclusion}

The single F-score objective over brackets or dependencies obscures important
differences between statistical parsers. For instance, a single attachment
error can lead to one or many mismatched brackets.

We have created a novel tree-transformation methodology for
evaluating parsers that categorises errors into linguistically meaningful
types.  Using this approach, we presented the first detailed examination of the
errors produced by a wide range of constituency parsers for
English.  We found that PP attachment and clause attachment are the most
challenging constructions, while coordination turns out to be less problematic
than previously thought.  We also noted interesting variations in error types
for parsers variants.

We investigated the errors resolved in reranking, and introduced by changing
domains. We found that the Charniak rerankers improved most error types, but
made little headway on improving PP attachment.  Changing domain has an impact
on all error types, except NP internal structure.

We have released our system so that future constituent parsers can be evaluated
using our methodology.  Our analysis provides new insight into the development
of parsers over the past fifteen years, and the challenges that remain.
