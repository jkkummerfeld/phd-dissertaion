\chapter{Automatic Error Analysis}

\begin{center}
\textit{
  Preliminary versions of this chapter appeared as \textcite{Kummerfeld-etal:2012:EMNLP} and \textcite{Kummerfeld-etal:2013:ACL}.
}
\end{center}

Constituency parser performance is primarily interpreted through a single metric, F-score on \wsj section 23, that conveys no linguistic information regarding the remaining errors.
In this chapter, we descibe a new error analysis method that classifies errors within a set of linguistically meaningful types using tree transformations that repair groups of errors together.
We use this analysis to answer a range of questions about parser behaviour, including what linguistic constructions are difficult for state-of-the-art parsers, what types of errors are being resolved by rerankers, and what types are introduced when parsing out-of-domain text.

%%% TODO:
%%% - Add 'treebank', 'parser' sideways labels to figures
%%%	Something else to think about - this paper is about 'what',
%%%	not 'why', so we don't use concepts like subcategorisation
%%%	Errors related to temporal structures would be a sensible
%%%	part though

\section{Parsers}

Our evaluation is over a wide range of \ptb constituency parsers and their variants from the past fifteen years.
For all parsers we used the publicly available version, with the standard parameter settings.

\begin{description}
	\item[ Berkeley] \parencite{Petrov-etal:2006,Petrov-Klein:2007}. An
	unlexicalised parser with a grammar constructed with automatic state
	splitting.

	\item[ Bikel] (\citeyear{Bikel:2004}) implementation of \textcite{Collins:1997}.

	\item[ BUBS]
	\parencite{Dunlop-Bodenstab-Roark:2011,Bodenstab-Dunlop-Hall-Roark:2011}. A
	`grammar-agnostic constituent parser,' which uses a Berkeley Parser grammar,
	but parses with various pruning techniques to improve speed, at the cost of
	accuracy.

	\item[ Charniak] (\citeyear{Charniak:2000}). A generative parser with a
	maximum entropy-inspired model.  We also use the reranker
	\parencite{Charniak-Johnson:2005}, and the self-trained model
	\parencite{McClosky-Charniak-Johnson:2006}.

	\item[ Collins] (\citeyear{Collins:1997}). A generative lexicalised parser, with
	three models, a base model, a model that uses subcategorisation
	frames for head words, and a model that takes into account traces.

	\item[ SSN] \parencite{Henderson:2003,Henderson:2004}. A statistical left-corner
	parser, with probabilities estimated by a neural network.

	\item[ Stanford] \parencite{Klein-Manning:2003:ACL,Klein-Manning:2003:NIPS}. We
	consider both the unlexicalised PCFG parser (-U) and the factored parser
	(-F), which combines the PCFG parser with a lexicalised dependency parser.
\end{description}

\begin{table}
\begin{center}
\begin{tabular}{|lccccr|}
	\hline
		System & F & P & R & Exact & Speed \\
	\hline
	\hline
		\multicolumn{6}{|c|}{Enhanced Training / Systems} \\
		Charniak-SR & 92.07 & 92.44 & 91.70 & 44.87 & 1.8 \\
		Charniak-R & 91.41 & 91.78 & 91.04 & 44.04 & 1.8 \\
		Charniak-S & 91.02 & 91.16 & 90.89 & 40.77 & 1.8 \\
	\hline
		\multicolumn{6}{|c|}{Standard Parsers} \\
		Berkeley & 90.06 & 90.30 & 89.81 & 36.59 & 4.2 \\
		Charniak & 89.71 & 89.88 & 89.55 & 37.25 & 1.8 \\
		SSN & 89.42 & 89.96 & 88.89 & 32.74 & 1.8 \\
		BUBS & 88.50 & 88.57 & 88.43 & 31.62 & 27.6 \\
		Bikel & 88.16 & 88.23 & 88.10 & 32.33 & 0.8 \\
		Collins-3 & 87.66 & 87.82 & 87.50 & 32.22 & 2.0 \\
		Collins-2 & 87.62 & 87.77 & 87.48 & 32.51 & 2.2 \\
		Collins-1 & 87.09 & 87.29 & 86.90 & 30.35 & 3.3 \\
		Stanford-L & 86.42 & 86.35 & 86.49 & 27.65 & 0.7 \\
		Stanford-U & 85.78 & 86.48 & 85.09 & 28.35 & 2.7 \\
	\hline
\end{tabular}
\caption{
	\label{tab:standard-results} \parseval results on \wsj section 23 for
	the parsers we consider.  The columns are F-score, precision, recall,
	exact sentence match, and speed (sents/sec).  Coverage was left out
	as it was above 99.8\% for all parsers.  In the {enhanced training /
	systems} section we include the Charniak parser with reranking (R), with a
	self-trained model (S), and both (SR).
}
\end{center}
\end{table}

Table~\ref{tab:standard-results} shows the standard performance metrics,
measured on section 23 of the \wsj, using all sentences.  Speeds were measured
using a Quad-Core Xeon CPU (2.33GHz 4MB L2 cache) with 16GB of RAM.
These results clearly show the variation in parsing performance, but they do
not show which constructions are the source of those variations.

\begin{figure}
\begin{center}
\scalebox{\derivscale}{
\synttree
[S
	[NP
		[PRP [He]]]
	[VP
		[VBD [was]]
		[VP
			[VBN [named]]
			[\wrongnode{S}
				[\wrongnode{NP}
					[NP [.t chief executive officer]]
					[\wrongnode{PP}
						[IN [of]]
						[\wrongnode{NP}
							[NP
								[NNP [Applied]]]
							[PP [.t in 1986]]]]]]]]]
}

(a) Parser output
\vspace{3mm}

\scalebox{\derivscale}{
\synttree
[S
	[NP
		[PRP [He]]]
	[VP
		[VBD [was]]
		[VP
			[VBN [named]]
			[\wrongnode{S}
				[\wrongnode{NP}
					[NP [.t chief executive officer]]
					[\wrongnode{PP}
						[IN [of]]
						[NP
							[NNP [Applied]]]]]]
			[PP [.t in 1986]]]]]
}

(b) Gold tree
\end{center}
\derivspace
\caption{
	\label{fig:PP-attachment}
	Grouping errors by node type is of limited usefulness.  In this figure and
	those that follow the top tree is the incorrect parse and the bottom tree is
	the correct parse.  Bold, boxed nodes are either extra (marked in the
	incorrect tree) or missing (marked in the correct tree).  This is an example
	of \textbf{PP~Attachment} (\emph{in~1986} is too low), but that is not at all
	clear from the set of incorrect nodes (extra S, NP, PP, and NP, missing S,
	NP, and PP).
}
\derivaftercompress
\end{figure}

\section{Error Classification}

While the statistics in Table~\ref{tab:standard-results} give a sense of
overall parser performance they do not provide linguistically meaningful
intuition for the source of remaining errors.  Breaking down the remaining
errors by node type is not particularly informative, as a single attachment
error can cause multiple node errors, many of which are for unrelated node
types.  For example, in Figure~\ref{fig:PP-attachment} there is a PP attachment
error that causes seven bracket errors (extra S, NP, PP, and NP, missing S,
NP, and PP).  Determining that these correspond to a PP attachment error from
just the labels of the missing and extra nodes is difficult.  In contrast, the
approach we describe below takes into consideration the relations between
errors, grouping them into linguistically meaningful sets.

We classify node errors in two phases.  First, we find a set of tree
transformations that convert the output tree into the gold tree.  Second, the
transformation are classified into error types such as PP attachment and
coordination.  Pseudocode for our method is shown in Algorithm~\ref{alg:code}.
The tree transformation stage corresponds to the main loop, while the second
stage corresponds to the final loop.

\begin{algorithm}[t]
\begin{algorithmic}
\State $U =$ initial set of node errors
\State Sort $U$ by the depth of the error in the tree, deepest first
\State $G = \emptyset$
\Repeat
	\ForAll{errors $e \in U$}
		\If{$e$ fits an environment template $t$}
			\State $g =$ new error group
			\State Correct $e$ as specified by $t$
			\ForAll{errors $f$ that $t$ corrects}
				\State Remove $f$ from $U$
				\State Insert $f$ into $g$
			\EndFor
			\State Add $g$ to $G$
		\EndIf
	\EndFor
\Until{unable to correct any further errors}
\ForAll{remaining errors $e \in U$}
	\State Insert a group into $G$ containing $e$
\EndFor
\ForAll{groups $g \in G$}
	\State Classify $g$ based on properties of the group
\EndFor
\end{algorithmic}
\caption{
\label{alg:code}
	Tree transformation error classification
}
\end{algorithm}

\subsection{Tree Transformation}

The core of our transformation process is a set of operations that move
subtrees, create nodes, and delete nodes.  Searching for the shortest path to
transform one tree into another is prohibitively slow.\footnote{We implemented
various search procedures and found similar results on the sentences that could
be processed in a reasonable amount of time.}  We find a path by applying a
greedy bottom--up approach, iterating through the errors in order of tree depth.
%%%in the tree, deepest first.

We match each error with a template based on nearby tree structure and
errors.  For example, in Figure~\ref{fig:PP-attachment} there are four extra
nodes that all cover spans ending at \emph{Applied in 1986}: S, NP, PP, NP.
There are also three missing nodes with spans ending between \emph{Applied} and
\emph{in}: PP, NP, and S.  Figure~\ref{fig:template} depicts these errors as
spans, showing that this case fits three criteria: (1) there are a set of extra
spans all ending at the same point, (2) there are a set of missing spans all
ending at the same point, and (3) the extra spans cross the missing spans,
extending beyond their end-point.  This indicates that the node starting after
\emph{Applied} is attaching too low and should be moved up, outside all of the
extra nodes.  Together, the criteria and transformation form a template.

\begin{figure}
\begin{center}
% TODO: Make figure
%%%\begin{pspicture}(-0.07,-0.6)(7.9,0.6)
%%%\psline(1.02,0.16)(1.02,0.75)(7.85,0.75)(7.85,0.16)
%%%\psline(4.52,0.16)(4.52,0.65)(7.75,0.65)(7.75,0.16)
%%%\psline(5.03,0.16)(5.03,0.55)(7.65,0.55)(7.65,0.16)
%%%\psline[linestyle=dashed,dash=4pt 2pt](1.02,0.02)(1.02,-0.51)(6.35,-0.51)(6.35,0.02)
%%%\psline[linestyle=dashed,dash=4pt 2pt](4.52,0.02)(4.52,-0.41)(6.25,-0.41)(6.25,0.02)
%%%\small named$\;$ chief$\;$ executive$\;$ officer$\;$ of$\;$ Applied$\;$ in$\;$ 1986
%%%\end{pspicture}
\end{center}
\caption{
	\label{fig:template}
	Templates are defined in terms of extra and missing spans, shown here with
	unbroken lines above and dashed lines below, respectively.  This is an
	example of a set of extra spans that cross a set of missing spans (which in
	both cases all end at the same position).  If the last two words are moved,
	two of the extra spans will match the two missing spans.  The other extra
	span is deleted during the move as it creates an NP$\rightarrow$NP unary
	production.
}
\end{figure}

Once a suitable template is identified we correct the error by moving subtrees,
adding nodes and removing nodes.  In the example this is done by moving the
node spanning \emph{in 1986} up in the tree until it is outside of all the
extra spans.  Since moving the PP leaves a unary production from an NP to an
NP, we also collapse that level.  In total this corrects seven errors, as there
are three cases in which an extra node is present that matches a missing node
once the PP is moved.  All of these errors are placed in a single group and
information about the nearby tree structure before and after the transformation
is recorded.

We continue to make passes through the list until no errors are corrected on a
pass.  For each remaining node error an individual error group is created.

The templates were constructed by hand based on manual analysis of parser
output.  They cover a range of combinations of extra and missing spans, with
further variation for whether crossing is occurring and if so whether the
crossing bracket starts or ends in the middle of the correct bracket.
Errors that do not match any of our templates are left uncorrected.

\subsection{Transformation Classification}

We began with a large set of node errors, in the first stage they were placed
into groups, one group per tree transformation used to get from the test tree
to the gold tree.  Next we classify each group as one of the error types below.

\paragraph{PP Attachment} Any case in which the transformation involved moving a Prepositional Phrase, or the incorrect bracket is over a PP, \myeg \\ 
\emph{He was} (VP \emph{named chief executive officer of} (NP \emph{Applied} (PP \emph{in 1986}))) \\
where (PP \emph{in 1986}) should modify the entire VP, rather than just \emph{Applied}.

\begin{figure}
\begin{center}
\scalebox{\derivscale}{
\synttree
[VP
	[VBD [wrote]]
		[\wrongnode{NP}
			[NP
				[DT [another]]
				[JJ [new]]
				[NN [ad]]]
			[VBG [appearing]]]
		[NP
			[NN [today]]]]
}

\vspace{3mm}
(a) Parser output

\vspace{3mm}

\scalebox{\derivscale}{
\synttree
[VP
	[VBD [wrote]]
		[\wrongnode{NP}
			[NP
				[DT [another]]
				[JJ [new]]
				[NN [ad]]]
			[\wrongnode{VP}
				[VBG [appearing]]
				[NP
					[NN [today]]]]]]
}

(b) Gold tree
\end{center}
\derivspace
\caption{
	\label{fig:NP-attachment}
	\textbf{NP Attachment}: \emph{today} is too high, it should be the argument
	of \emph{appearing}, rather than \emph{wrote}.  This causes three node errors
	(extra NP, missing NP and VP).
}
\derivaftercompress
\end{figure}

\paragraph{NP Attachment} Several cases in which NPs had to be moved, particularly for mistakes in appositive constructions and incorrect attachments within a verb phrase, \myeg \\
\emph{The bonds} (VP \emph{go} (PP \emph{on sale} (NP \emph{Oct.\@\xspace 19}))) \\
where \emph{Oct.\@\xspace 19} should be an argument of \emph{go}.

\begin{figure}
\begin{center}
\scalebox{\derivscale}{
\synttree
[VP
	[VBD [had]]
	[\wrongnode{S}
		[\wrongnode{VP}
			[TO [to]]
			[\wrongnode{VP}
				[VB [think]]
				[PP [.t about it]]]]]
	[ADVP [.t ahead of time]]]
}

\vspace{3mm}
(a) Parser output

\vspace{6mm}

\scalebox{\derivscale}{
\synttree
[VP
	[VBD [had]]
	[\wrongnode{S}
		[\wrongnode{VP}
			[TO [to]]
			[\wrongnode{VP}
				[VB [think]]
				[PP [.t about it]]
				[ADVP [.t ahead of time]]]]]]
}

\vspace{3mm}
(b) Gold tree
\end{center}
\derivspace
\caption{
	\label{fig:modifier-attachment}
	\textbf{Modifier Attachment}: \emph{ahead of time} is too high, it should
	modify \emph{think}, not \emph{had}.  This causes six node errors (extra
	S, VP, and VP, missing S, VP, and VP).
}
\derivaftercompress
\end{figure}

\paragraph{Modifier Attachment} Cases involving incorrectly placed adjectives
and adverbs, including errors corrected by subtree movement and errors
requiring only creation of a node, \myeg \\ (NP (ADVP \emph{even more})
\emph{severe setbacks}) \\ where there should be an extra ADVP node over
\emph{even more severe}.

\begin{figure}
\begin{center}
\scalebox{\derivscale}{
\synttree
[VP
	[VBZ [intends]]
	[\wrongnode{S}
		[\wrongnode{VP}
			[TO [to]]
			[\wrongnode{VP}
				[VB [restrict]]
				[NP [.t the RTC to \ldots]]
				[SBAR [.t unless the agency \ldots]]]]]]
}

\vspace{3mm}
(a) Parser output

\vspace{6mm}

\scalebox{\derivscale}{
\synttree
[VP
	[VBZ [intends]]
	[\wrongnode{S}
		[\wrongnode{VP}
			[TO [to]]
			[\wrongnode{VP}
				[VB [restrict]]
				[NP [.t the RTC to \ldots]]]]]
	[SBAR [.t unless the agency \ldots]]]
}

\vspace{3mm}
(b) Gold tree
\end{center}
\derivspace
\caption{
	\label{fig:clause-attachment}
	\textbf{Clause Attachment}: \emph{unless the agency receives specific
	congressional authorization} is attaching too low. This causes six
	node errors (extra S, VP, and VP, missing S, VP and VP).
}
\derivaftercompress
\end{figure}

\paragraph{Clause Attachment}
Any group that involves movement of some form of S node.

\begin{figure}
\begin{center}
\scalebox{\derivscale}{
\synttree
[SINV
	[VP
		[VBG [Following]]]
	[VBZ [is]]
	[NP
		[NP [.t a breakdown]]
		[PP [.t of major market activity]]]]
}

\vspace{3mm}
(a) Parser output

\vspace{6mm}

\scalebox{\derivscale}{
\synttree
[SINV
	[\wrongnode{S}
		[VP
			[VBG [Following]]]]
	[VBZ [is]]
	[\wrongnode{NP}
		[NP
			[NP [.t a breakdown]]
			[PP [.t of major market activity]]]]]
}

\vspace{3mm}
(b) Gold tree

\vspace{6mm}

\scalebox{\derivscale}{
\synttree
[SINV
	[S-ADV
		[NP-SBJ
			[-NONE- [*-1]] ]
		[VP
			[VBG [Following]]]]
	[VBZ [is]]
	[NP-SBJ-1
		[NP
			[NP [.t a breakdown]]
			[PP [.t of major market activity]]]]
	[$\colon$ [$\colon$]]]
}

\vspace{3mm}
(c) Gold tree with traces and function tags
\end{center}
\derivspace
\caption{
	\label{fig:unary}
	Two \textbf{Unary} errors, a missing S and a missing NP.  The third tree is
	the \ptb tree before traces and function tags are removed.  Note that the
	missing NP is over another NP, a production that does occur widely in the
	treebank, particularly over the word \emph{it}.
}
\derivaftercompress
\end{figure}

\paragraph{Unary}
Mistakes involving unary productions that are not linked to a nearby error  such as a matching extra or missing node.
We do not include a breakdown by unary type, though we did find that clause labeling (S, SINV, etc) accounted for a large proportion of the errors.

\begin{figure}
\begin{center}
\scalebox{\derivscale}{
\synttree
[NP
	[NP [.t A 16\% drop]]
	[\wrongnode{PP}
		[IN [for]]
		[\wrongnode{NP}
			[NP [.t Mannesmann AG]]
			[CC [and]]
			[NP [.t Dresdner AG's 10\% decline]]]]]
}

\vspace{3mm}
(a) Parser output

\vspace{6mm}

\scalebox{\derivscale}{
\synttree
[NP
	[\wrongnode{NP}
		[NP [.t A 16\% drop]]
		[\wrongnode{PP}
			[IN [for]]
			[NP [.t Mannesmann AG]]]]
	[CC [and]]
	[NP [.t Dresdner AG's 10\% decline]]]
}

\vspace{3mm}
(b) Gold tree
\end{center}
\derivspace
\caption{
	\label{fig:coordination}
	\textbf{Coordination}: \emph{and Dresdner AG's 10\% decline} is too low.
	This causes four node errors (extra PP and NP, missing NP and PP).
}
\derivaftercompress
\end{figure}

\paragraph{Coordination}
Cases in which a conjunction is an immediate sibling of the nodes being moved, or is the leftmost or rightmost node being moved.

\begin{figure}
\begin{center}
\scalebox{\derivscale}{
\synttree
[NP
	[NNP [Secretary]]
	[\wrongnode{PP}
		[IN [of]]
		[\wrongnode{NP}
			[NNP [State]]
			[NNP [Baker]]]]]
}

\vspace{3mm}
(a) Parser output

\vspace{6mm}

\scalebox{\derivscale}{
\synttree
[NP
	[NNP [Secretary]]
	[\wrongnode{PP}
		[IN [of]]
		[\wrongnode{NP}
			[NNP [State]]]]
	[NNP [Baker]]]
}

\vspace{3mm}
(b) Gold tree
\end{center}
\derivspace
\caption{
	\label{fig:NP-internal-structure}
	\textbf{NP Internal Structure}: \emph{Baker} is too low, causing four
	errors (extra PP and NP, missing PP and NP).
}
\derivaftercompress
\end{figure}

\paragraph{NP Internal Structure}
While most NP structure is not annotated in the \ptb, there is some use of ADJP, NX, NAC and QP nodes.
We form a single group for each NP that has one or more errors involving these types of nodes.

\paragraph{Different label}
In many cases a node is present in the tree that spans the correct set of words, but has the wrong label, in which case we group the two node errors, (one extra, one missing), as a single error.

\paragraph{Single word phrase}
A range of node errors that span a single word, with checks to ensure this is not linked to another error (\myeg one part of a set of internal noun phrase errors).

\paragraph{Other}
There is a long tail of other errors.
Some could be placed within the categories above, but would require far more specific rules.

For many of these error types it would be difficult to extract a meaningful understanding from only the list of node errors involved.
Even for error types that can be measured by counting node errors or rule production errors, our approach has the advantage that we identify groups of errors with a single cause.
For example, a missing unary production may correspond to an extra bracket that contains a subtree that attached incorrectly.

\subsection{Methodology}

We used sections 00 and 24 as development data while constructing the tree transformation and error group classification methods.
All of our examples in text come from these sections as well, but for all tables of results we ran our system on section 23.
We chose to run our analysis on section 23 as it is the only section we are sure was not used in the development of any of the parsers, either for tuning or feature development.
Our evaluation is entirely focused on the errors of the parsers, so unless there is a particular construction that is unusually prevalent in section 23, we are not revealing any information about the test set that could bias future work.

\section{Results}

\begin{table}
\begin{center}
\begin{tabular}{|lrrr|}
	\hline
		                        &             & Nodes    &       \\
		Error Type\hspace{-8mm} & Occurrences & Involved & Ratio \\
	\hline
	\hline
		PP Attachment\hspace{-8mm} & 846 & 1455 & 1.7 \\
		Single word phrase\hspace{-8mm} & 490 & 490 & 1.0 \\
		Clause Attachment\hspace{-8mm} & 385 & 913 & 2.4 \\
		Modifier Attachment\hspace{-8mm} & 383 & 599 & 1.6 \\
		Different Label\hspace{-8mm} & 377 & 754 & 2.0 \\
		Unary & 347 & 349 & 1.0 \\
		NP Attachment\hspace{-8mm} & 321 & 597 & 1.9 \\
		NP Internal Structure\hspace{-8mm} & 299 & 352 & 1.2 \\
		Coordination & 209 & 557 & 2.7 \\
		Unary Clause Label\hspace{-8mm} & 185 & 200 & 1.1 \\
		VP Attachment & 64 & 159 & 2.5 \\
		Parenthetical Attachment\hspace{-8mm} & 31 & 74 & 2.4 \\
		Missing Parenthetical\hspace{-8mm} & 12 & 17 & 1.4 \\
		Unclassified & 655 & 734 & 1.1 \\
	\hline
\end{tabular}
\caption{\label{tab:charniak-breakdown}
	Breakdown of errors on section 23 for the Charniak parser with self-trained
	model and reranker.  Errors are sorted by the number of times they occur.
	Ratio is the average number of node errors caused by each error we identify
	(\myie Nodes Involved / Occurrences).
}
\end{center}
\end{table}

Our system enables us to answer questions about parser behaviour that could previously only be probed indirectly.
We demonstrate its usefulness by applying it to a range of parsers (here), to reranked K-best lists of various lengths, and to output for out-of-domain parsing (following sections).

\begin{landscape}
\input{figures/wsj23_comparison.tex}

\input{figures/reranking.tex}

\input{figures/domains_charniak.tex}
\end{landscape}

In Table~\ref{tab:wsj23-comp} we consider the breakdown of parser errors on \wsj section 23.
The shaded area of each bar indicates the frequency of parse errors (\myie empty means fewest errors).
The area filled in is determined by the expected number of node errors per sentence that are attributed to that type of error.
The average number of node errors per sentence for a completely full bar is indicated by the Worst row, and the value for a completely empty bar is indicated by the Best row.

We use counts of node errors to make the contributions of each type of error more interpretable.
As Table~\ref{tab:charniak-breakdown} shows, some errors typically cause only a single node error, where as others, such as coordination, generally cause several.
This means that considering counts of error groups would over-emphasise some error types, \myeg single word phrase errors are second most important by number of groups (in Table~\ref{tab:charniak-breakdown}), but seventh by total number of node errors (in Table~\ref{tab:wsj23-comp}).

As expected, PP attachment is the largest contributor to errors, across all parsers.
Interestingly, coordination is sixth on the list, though that is partly due to the fact that there are fewer coordination decisions to be made in the treebank.\footnote{This is indicated by the frequency of CCs and PPs in sections 02--21 of the treebank, 16,844 and 95,581 respectively.
These counts are only an indicator of the number of decisions as the nodes can be used in ways that do not involve a decision, such as sentences that start with a conjunction.}

By looking at the performance of the Collins parser we can see the development
over the past fifteen years.  There has been improvement across
the board, but in some cases, \myeg clause attachment errors and different label
errors, the change has been more limited (24\% and 29\% reductions respectively).  We
investigated the breakdown of the different label errors by label, but no
particular cases of label confusion stand out, and we found that the most
common cases remained the same between Collins and the top results.

It is also interesting to compare pairs of parsers that share aspects of their
architecture.  One such pair is the Stanford parser, where the factored parser
combines the unlexicalised parser with a lexicalised dependency parser.  The
main sources of the 0.64 gain in F-score are PP attachment and coordination.

Another interesting pair is the Berkeley parser and the BUBS parser, which uses
a Berkeley grammar, but improves speed by pruning.  The pruning methods used in
BUBS are particularly damaging for PP attachment errors and unary errors.
%%%Performing experiments varying the parameters of their pruning method is beyond
%%%the scope of this paper, but if these experiments were run our system could be
%%%applied directly to the output to determine which parameters are causing this
%%%behaviour.

Various comparisons can be made between Charniak parser variants.  We discuss
the reranker below.  For the self-trained model
\textcite{McClosky-Charniak-Johnson:2006} performed some error analysis,
considering variations in F-score depending on the frequency of tags such as
PP, IN and CC in sentences.  Here we see gains on all error types, though
particularly for clause attachment, modifier attachment and coordination, which
fits with their observations.

\subsection{Reranking}

The standard dynamic programming approach to parsing limits the range of
features that can be employed.  One way to deal with this issue is to modify
the parser to produce the top $K$ parses, rather than just the 1-best, then use
a model with more sophisticated features to choose the best parse from this
list \parencite{collins:00}.  While re-ranking has led to gains in performance
\parencite{Charniak-Johnson:2005}, there has been limited analysis of how
effectively rerankers are using the set of available options.  Recent work has
explored this question in more depth, but focusing on how variation in the
parameters impacts performance on standard metrics
\parencite{huang:08a,Ng-etal:2010,Auli-Lopez:2011,Ng-Curran:2012}.

In Table~\ref{tab:reranking} we present a breakdown over error types for the
Charniak parser, using the self-trained model and reranker.  The oracle results
use the parse in each K-best list with the highest F-score.  While this may not
give the true oracle result, as F-score does not factor over sentences, it
gives a close approximation.  The table has the same columns as
Table~\ref{tab:wsj23-comp}, but the ranges on the bars now reflect the min and
max for these sets.

While there is improvement on all errors when using the reranker, there is very
little additional gain beyond the first 5-10 parses.  Even for the oracle
results, most of the improvement occurs within the first 5-10 parses.  The
limited utility of extra parses for the reranker may be due to the importance
of the base parser output probability feature (which, by definition, decreases
within the K-best list).
%%%Another possibility is that there is less useful variation further down the K-best list.
%%%The utility is higher for the oracle, but
%%%we do not see greater improvement further down the list because 
%%%those parses
%%%will be combinations of a set of variations in the parse that change the
%%%model probability only slightly, rather than providing useful variation.

Interestingly, the oracle performance improves across all error types, even at
the 2-best level.  This indicates that the base parser model is not
particularly biased against a single error.  Focusing on the rows for $K=2$ we
can also see two interesting outliers.  The PP attachment improvement of the
oracle is considerably higher than that of the reranker, particularly compared
to the differences for other errors, suggesting that the reranker lacks the
features necessary to make the decision better than the parser.  The other
interesting outlier is NP internal structure, which continues to make
improvements for longer lists, unlike the other error types.

\subsection{Out-of-Domain}

Parsing performance drops considerably when shifting outside of the domain a
parser was trained on \parencite{Gildea:2001}.
\textcite{Clegg:2005:EIT:1626315.1626317} evaluated parsers qualitatively on
node types and rule productions.  \textcite{Bender:2011:PEO:2145432.2145479}
designed a Wikipedia test set to evaluate parsers on dependencies representing
ten specific linguistic phenomena.

To provide a deeper understanding of the errors arising when parsing outside of
the newswire domain, we analyse performance of the Charniak parser with
reranker and self-trained model on the eight parts of the Brown corpus
\parencite{ptb}, and two parts of the Google Web
corpus \parencite{Petrov:2012}.  Table~\ref{tab:domain-info} shows statistics for
the corpora.  The variation in average sentence lengths skew the results for
errors per sentence.  To handle this we divide by the number of words to
determine the results in Table~\ref{tab:charniak-domains}, rather than by the
number of sentences, as in previous figures.

\begin{table}
\begin{center}
\begin{tabular}{|llrr|}
	\hline
		Corpus & Description & Sentences & Av. Length \\
	\hline
	\hline
		\wsj 23 & Newswire & 2416 & 23.5 \\
		Brown F & Popular & 3164 & 23.4 \\
		Brown G & Biographies & 3279 & 25.5 \\
		Brown K & General & 3881 & 17.2 \\
		Brown L & Mystery & 3714 & 15.7 \\
		Brown M & Science & 881 & 16.6 \\
		Brown N & Adventure & 4415 & 16.0 \\
		Brown P & Romance & 3942 & 17.4 \\
		Brown R & Humour & 967 & 22.7 \\
		G-Web Blogs & Blogs & 1016 & 23.6 \\
		G-Web Email & E-mail & 2450 & 11.9 \\
	\hline
\end{tabular}
\caption{
	\label{tab:domain-info}
	Variation in size and contents of the domains we consider.	The variation in
	average sentence lengths skews the results for errors per sentences, and so
	in Table~\ref{tab:charniak-domains} we consider errors per word.
}
\end{center}
\end{table}

There are several interesting features in the table.  First, on the Brown
datasets, while the general trend is towards worse performance on all errors,
NP internal structure is a notable exception and in some cases PP attachment
and unaries are as well.

In the other errors we see similar patterns across the corpora, except humour
(Brown R), on which the parser is particularly bad at coordination and clause
attachment.  This makes sense, as the colloquial nature of the text includes
more unusual uses of conjunctions, for example:

\vspace{3mm}
\emph{She was a living doll and no mistake -- the ... }
\vspace{3mm}

Comparing the Brown corpora and the Google Web corpora, there are much larger
divergences.  We see a particularly large decrease in NP internal structure.
Looking at some of the instances of this error, it appears to be largely caused
by incorrect handling of structures such as URLs and phone numbers, which do
not appear in the \ptb.  There are also some more difficult cases, for example:

\vspace{3mm}
\emph{... going up for sale in the next month or do .}
\vspace{3mm}

\noindent where \emph{or do} is a QP.  This typographical error is extremely
difficult to handle for a parser trained only on well-formed text.

For e-mail there is a substantial drop on single word phrases.  Breaking the
errors down by label we found that the majority of the new errors are missing
or extra NPs over single words.  Here the main problem appears to be temporal
expressions, though there also appear to be a substantial number of errors that
are also at the POS level, such as when NNP is assigned to \emph{ta} in this
case:

\vspace{3mm}
\emph{... let you know that I 'm out ta here !}
\vspace{3mm}

Some of these issues, such as URL handling, could be resolved with suitable
training data.  Other issues, such as ungrammatical language and
unconventional use of words, pose a greater challenge.

%%%  ============================================================================
%%%  
%%%  This paper reports on the results of systematic analysis of errors
%%%  made by Chinese parsers.
%%%  
%%%  The experimental results are very interesting.
%%%  Though the basic methodology used in the paper builds directly
%%%  on a previous work, it is adapted well to Chinese through careful
%%%  analysis of error types.
%%%  
%%%  It would be nice to give more in-depth comparison with previous
%%%  findings on the difficulty of Chinese parsing, but I understand
%%%  the authors have already done well within the limited space.
%%%  
%%%  ============================================================================
%%%  
%%%  The paper is a thorough analysis of problems in Chinese parsing. I like it that
%%%  a variety of parsers were used. The paper is generally well written. The
%%%  results do not seem particularly surprising but it is good to have empirical
%%%  evidence to support them. The analysis and software should support future
%%%  research into the problematic areas.
%%%  However, I would suggest adding statistical significance tests, and reworking
%%%  Section 4. I understand that not all error categories can be discussed in
%%%  detail but the selection was unsatisfactory. Table 1 shows which error types
%%%  are new in this work, but not all of them are explained. It's not clear how the
%%%  order of subsections is determined. Some types are presented mostly to say that
%%%  they are less important than in English (PP attachment), while the second
%%%  biggest error type (coordination) is not discussed in this section at all, and
%%%  the third-highest only very briefly (not enough to make me understand why this
%%%  is substantially different from English).
%%%  Maybe for those categories that are comparable to Kummerfeld, you could add the
%%%  English percentages to Table 1?
%%%  I'm not so convinced by the final conclusion, NP internal structure is an issue
%%%  in English as well, just not in the PTB annotation. Coordination is still not
%%%  properly solved for English.
%%%  
%%%  ============================================================================
%%%  
%%%  This paper presents a comprehensive analysis of errors in Chinese Parsing,
%%%  and releases an open-source code for it.
%%%  
%%%  The author adapt the system from Kummerfeld et al.2012 to Chinese,
%%%  and examine a wide range of Chinese constitute Parsers.
%%%  Experimental results show a series of interesting findings,
%%%  mainly on verb-argument-attach-error and coordination errors.
%%%  The idea is clear and the paper is well written.
%%%  
%%%  What I miss is a more specific conclusion: I think it's not as deep as
%%%  Kummerfeld in 2012. Yes, empirically we can see the coord and verb args fields
%%%  are not quite improved even with gold POS tag. But I think it would be clearer
%%%  if the author can give some examples to explain why gold POS can't do that. And
%%%  this would help us to find the right direction to address these problems.

\section{Chinese Parsing}

Aspects of Chinese syntax result in a distinctive mix of parsing challenges.
However, the contribution of individual sources of error to overall difficulty is not well understood.  
We conduct a comprehensive automatic analysis of error types made by Chinese parsers, covering a broad range of error types for large sets of sentences, enabling the first empirical ranking of Chinese error types by their performance impact.  
To accommodate error classes that are absent in English, we augment the system to recognize Chinese-specific parse errors.
To understand the impact of tagging errors on different error types, we also performed a part-of-speech ablation experiment, in which particular confusions are introduced in isolation.
By analyzing the distribution of errors in the system output with and without gold part-of-speech tags, we are able to isolate and quantify the error types that can be resolved by improvements in tagging accuracy.
Our analysis shows that improvements in tagging accuracy can only address a subset of the challenges of Chinese syntax.
Further improvement in Chinese parsing performance will require research addressing other challenges, in particular, determining coordination scope.

\subsection{Background}

A decade of Chinese parsing research, enabled by the Penn Chinese Treebank \parencite[\pctb;][]{Xue:2005:NLE}, has seen Chinese parsing performance improve from 76.7 F$_1$ \parencite{Bikel-Chiang:2000:CLP} to 84.1 F$_1$ \parencite{Qian-Liu:2012:EMNLP}.
While recent advances have focused on understanding and reducing the errors that occur in segmentation and part-of-speech tagging \parencite{Qian-Liu:2012:EMNLP,Jiang-etal:2009:ACL,Forst-Fang:2009:EACL}, a range of substantial issues remain that are purely syntactic.

The closest previous work is the detailed manual analysis performed by \textcite{Levy-Manning:2003:ACL}.
While their focus was on issues faced by their factored PCFG parser \parencite{Klein-Manning:2003:NIPS}, the error types they identified are general issues presented by Chinese syntax in the \pctb.
They presented several Chinese error types that are rare or absent in English, including noun/verb ambiguity, NP-internal structure and coordination ambiguity due to \emph{pro}-drop.
They attributed some of these differences to treebank annotation decisions and others to meaningful differences in syntax, suggesting that closing the English-Chinese parsing gap demands techniques beyond those currently used for English.
Based on this analysis they considered how to modify their parser to capture the information necessary to model the syntax within the \pctb.
However, as noted in their final section, their manual analysis of parse errors in 100 sentences only covered a portion of a single parser's output, limiting the conclusions they could reach regarding the distribution of errors in Chinese parsing.

\subsection{Adapting Error Analysis to Chinese} \label{sec:adapting_automatic_error_analysis_to_chinese}

Our analysis builds on the system described above, which finds the shortest path from the system output to the gold annotations, then classifies each transformation step into one of several error types.
When directly applied to Chinese parser output, the system placed over 27\% of the errors in the catch-all `Other' type.
Many of these errors clearly fall into one of a small set of error types, motivating an adaptation to Chinese syntax.

To adapt the system to Chinese, we developed a new version of the second stage of the system, which assigns an error category to each tree transformation step.

To characterize the errors the original system placed in the `Other' category,
we looked through one hundred sentences, identifying error types generated by
Chinese syntax that the existing system did not account for.
With these observations we were able to implement new rules to catch the
previously missed cases, leading to the set shown in Table~\ref{tab:errors}.
To ensure the accuracy of our classifications, we alternated between refining
the classification code and looking at affected classifications to identify
issues.  We also periodically changed the sentences from the development set
we manually checked, to avoid over-fitting.

Where necessary, we also expanded the information available during
classification.  For example, we use the structure of the final gold standard
tree when classifying errors that are a byproduct of sense disambiguation
errors.

\subsection{Chinese Parsing Errors} \label{sec:chinese_parsing_errors}

\begin{table}
\centering
\begin{tabular}{lrr}
  \hline
  Error Type & Brackets & \% of total \\
  \hline
  \hline
              NP-internal* & 6019 & 22.70\% \\
              Coordination & 2781 & 10.49\% \\
   Verb taking wrong args* & 2310 &  8.71\% \\
                     Unary & 2262 &  8.53\% \\
       Modifier Attachment & 1900 &  7.17\% \\
             One Word Span & 1560 &  5.88\% \\
           Different label & 1418 &  5.35\% \\
            Unary A-over-A & 1208 &  4.56\% \\
   Wrong sense/bad attach* & 1018 &  3.84\% \\
      Noun boundary error* &  685 &  2.58\% \\
             VP Attachment &  626 &  2.36\% \\
         Clause Attachment &  542 &  2.04\% \\
             PP Attachment &  514 &  1.94\% \\
      Split Verb Compound* &  232 &  0.88\% \\
              Scope error* &  143 &  0.54\% \\
             NP Attachment &  109 &  0.41\% \\
                     Other & 3186 & 12.02\% \\
\hline
\end{tabular}
\caption{
    \label{tab:errors}
    Errors made when parsing Chinese. Values are the number of bracket errors
    attributed to that error type. The values shown are for the Berkeley parser,
		evaluated on the development set. * indicates error types that were added or
		substantially changed as part of this work.
}
\end{table}

Table~\ref{tab:errors} presents the errors made by the Berkeley parser.
Below we describe the error types that are either new in this analysis, have had their definition altered, or have an interesting distribution.

In all of our results we follow the same approach as earlier, presenting the number of bracket errors (missing or extra) attributed to each error type.
Bracket counts are more informative than a direct count of each error type, because the impact on EVALB F-score varies between errors, \myeg a single attachment error can cause 20 bracket errors, while a unary error causes only one.

\begin{figure}
\centering
  \scalebox{1.2}{ \includegraphics{figures/chinese-tree2} }
  \caption{NP-internal structure errors, gold structure (left) and parser hypothesis (right).}\label{fig:np_internal}
\end{figure}

\paragraph{NP-internal} (Figure~\ref{fig:np_internal})
Unlike the Penn Treebank \parencite{ptb}, the \pctb annotates some NP-internal structure.
We assign this error type when a transformation involves words whose parts of speech in the gold tree are one of: CC, CD, DEG, ETC, JJ, NN, NR, NT and OD.

We investigated the errors that fall into the NP-internal category and found that 49\% of the errors involved the creation or deletion of a single pre-termianl phrasal bracket.
These errors arise when a parser proposes a tree in which POS tags (for instance, JJ or NN) occur as siblings of phrasal tags (such as NP), a configuration used by the PCTB bracketing guidelines to indicate complementation as opposed to adjunction \parencite{Xue:2005:NLE}.

\begin{figure}
\centering
  \scalebox{1.2}{ \includegraphics{figures/chinese-treebush} }
  \caption{Verb taking wrong arguments, gold structure (left) and parser hypothesis (right).}\label{fig:wrong_arg}
\end{figure}

\paragraph{Verb taking wrong args} (Figure~\ref{fig:wrong_arg})
This error type
arises when a verb \mbox{(\myeg~\glos{扭转}{reverse})} is hypothesized to take
an incorrect argument (\mbox{\glos{布什}{Bush}} instead of
\mbox{\glos{地位}{position}}).  Note that this also covers some of the errors
that were classified as NP Attachment for English, changing
the distribution for that type.

\paragraph{Unary}
For mis-application of unary rules we separate out instances in which the two brackets in the production have the the same label (A-over-A).
This cases is created when traces are eliminated, a standard step in evaluation.
More than a third of unary errors made by the Berkeley parser are of the A-over-A type.
This can be attributed to two factors: (i) the \pctb annotates non-local dependencies using traces, and (ii) Chinese syntax generates more traces than English syntax, such as \emph{pro}-drop, the omission of arguments where the referent is recoverable from discourse \parencite{Guo-Wang-VanGenabith:2007:EMNLP}.
However, for parsers that do not return traces they are a benign error.

\begin{figure}
\centering
  \scalebox{1.2}{ \includegraphics{figures/chinese-treemodatt} }
  \caption{Modifier attachment ambiguity, gold structure (left) and parser hypothesis (right).}\label{fig:mod_att}
\end{figure}

\paragraph{Modifier attachment} (Figure~\ref{fig:mod_att})  Incorrect modifier
scope caused by modifier phrase attachment level. This is less frequent in
Chinese than in English: while English VP modifiers occur in pre- and
post-verbal positions, Chinese only allows pre-verbal modification.

\begin{figure}
\centering
  \scalebox{1.2}{ \includegraphics{figures/chinese-tree1} }
  \caption{Sense confusion, gold structure (left) and parser hypothesis (right).}\label{fig:sense}
\end{figure}

\paragraph{Wrong sense/bad attach} (Figure~\ref{fig:sense})
This applies
when the head word of a phrase receives the wrong POS, leading to an attachment
error.  This error type is common in Chinese because of POS fluidity, \myeg the
well-known Chinese verb/noun ambiguity often causes mis-attachments that are
classified as this error type.

In Figure~\ref{fig:sense}, the word \mbox{\glos{投资}{invest}} has both noun and
verb senses. While the gold standard interpretation is the relative clause
\mbox{\Trans{firms that Macau invests in}}, the parser returned an NP
interpretation \mbox{\Trans{Macau investment firms}}.

\paragraph{Noun boundary error}  In this error type, a span is moved to a
position where the POS tags of its new siblings all belong to the
list of NP-internal structure tags which we identified above, reflecting
the inclusion of additional material into an NP.

\paragraph{Split verb compound} The \pctb annotations recognize several
Chinese verb compounding strategies, such as the serial verb construction
(\mbox{\glos{规划建设}{plan [and] build}}) and the resultative construction
(\mbox{\glos{煮熟}{cook [until] done}}), which join a bare verb to another
lexical item.  We introduce an error type specific to Chinese, in which such
verb compounds are split, with the two halves of the compound placed in
different phrases.

\paragraph{Scope error}
These are cases in which a new span must be added to
more closely bind a modifier phrase (ADVP, ADJP, and PP).

\paragraph{PP attachment} 
This error type is rare in Chinese, as adjunct PPs are pre-verbal.  It does
occur near coordinated VPs, where ambiguity arises about which of the conjuncts
the PP has scope over.  Whether this particular case is PP attachment or
coordination is debatable; we follow the approach above and
label it PP attachment.

\begin{landscape}
\begin{table*}
\centering
\setlength\fboxsep{0mm}
\setlength\fboxrule{0.05mm}
\begin{tabular}{|lccccccccccccc|}
	\hline
	    & NP &  & Verb &  & Mod. & 1-Word & Diff & Wrong & Noun & VP & Clause & PP & \\
	System \hfill F$_1$ & Int. & Coord & Args & Unary & Attach & Span & Label & Sense & Edge & Attach & Attach & Attach & Other \\
	\hline
	\hline
	\emph{Best}    & 1.54 & 1.25 & 1.01 & 0.76 & 0.72 & 0.21 & 0.30 & 0.05 & 0.21 & 0.26 & 0.22 & 0.18 & 1.87 \\
	Berk-G$\;$ \hfill 86.8 &  \mybar{3.117775} &  \mybar{5.719997} &  \mybar{4.868075} &  \mybar{4.092204} &  \mybar{3.414187} &  \mybar{1.560478} &  \mybar{2.304226} &  \mybar{0.417061} &  \mybar{3.003337} &  \mybar{4.124765} &  \mybar{4.199998} &  \mybar{3.250795} &  \mybar{3.627305} \\
	Berk-2 \hfill 81.8 &  \mybar{5.106060} &  \mybar{5.726296} &  \mybar{4.657457} &  \mybar{5.602480} &  \mybar{3.889412} &  \mybar{5.874318} &  \mybar{5.465338} &  \mybar{4.259078} &  \mybar{4.180596} &  \mybar{4.645864} &  \mybar{3.974996} &  \mybar{4.055024} &  \mybar{4.120634} \\
	Berk-1 \hfill 81.1 &  \mybar{5.637080} &  \mybar{5.854733} &  \mybar{4.914895} &  \mybar{5.637384} &  \mybar{4.160972} &  \mybar{5.409644} &  \mybar{5.105299} &  \mybar{4.288569} &  \mybar{4.581930} &  \mybar{4.594485} &  \mybar{4.516671} &  \mybar{4.351321} &  \mybar{4.375378} \\
	ZPAR \hfill 78.1 &  \mybar{5.603375} &  \mybar{6.374733} &  \mybar{5.487246} &  \mybar{7.386910} &  \mybar{5.851629} &  \mybar{5.863920} &  \mybar{6.833466} &  \mybar{4.423375} &  \mybar{5.725748} &  \mybar{5.299080} &  \mybar{6.791682} &  \mybar{6.653960} &  \mybar{5.833838} \\
	Bikel \hfill 76.1 &  \mybar{6.464990} &  \mybar{7.357862} &  \mybar{6.227640} &  \mybar{6.372585} &  \mybar{6.280885} &  \mybar{6.602543} &  \mybar{5.609351} &  \mybar{6.904680} &  \mybar{8.000000} &  \mybar{5.790822} &  \mybar{6.700012} &  \mybar{6.484650} &  \mybar{5.983631} \\
	Stan-F \hfill 76.0 &  \mybar{6.824621} &  \mybar{8.000000} &  \mybar{6.340422} &  \mybar{6.721463} &  \mybar{6.291805} &  \mybar{6.949312} &  \mybar{6.977479} &  \mybar{4.827803} &  \mybar{5.886282} &  \mybar{7.060545} &  \mybar{7.166674} &  \mybar{6.933336} &  \mybar{5.501970} \\
	Stan-P \hfill 70.0 &  \mybar{8.000000} &  \mybar{7.035766} &  \mybar{8.000000} &  \mybar{8.000000} &  \mybar{8.000000} &  \mybar{8.000000} &  \mybar{8.000000} &  \mybar{8.000000} &  \mybar{7.464874} &  \mybar{8.000000} &  \mybar{8.000000} &  \mybar{8.000000} &  \mybar{8.000000} \\
	\emph{Worst}   & 3.94 & 1.75 & 1.73 & 1.48 & 1.68 & 1.06 & 1.02 & 0.88 & 0.55 & 0.50 & 0.44 & 0.44 & 4.11 \\
	\hline
\end{tabular}
\caption{\label{tab:comparison}
Error breakdown for the development set of \pctb 6.  The area filled in for
each bar indicates the average number of bracket errors per sentence attributed
to that error type, where an empty bar is no errors and a full bar has
the value indicated in the bottom row.  The parsers are:
the Berkeley parser with gold POS tags as input,
the Berkeley parser,
\parencite[Berk-G and Berk-1][]{Petrov-etal:2006,Petrov-Klein:2007},
the Berkeley product parser with two grammars \parencite[Berk-2][]{Petrov:2010:NAACLHLT},
the parser of \textcite[][ZPAR]{Zhang-Clark:2009:ICPT},
the Bikel parser \parencite[Bikel][]{Bikel-Chiang:2000:CLP},
the Stanford Factored parser \parencite[Stan-F][]{Levy-Manning:2003:ACL,Klein-Manning:2003:NIPS},
and the Stanford Unlexicalized PCFG parser \parencite[Stan-P][]{Klein-Manning:2003:ACL}.
}
\end{table*}
\end{landscape}

\subsection{Chinese-English Comparison} \label{subsec:chinese_english_comparison}

It is difficult to directly compare error analysis results for Chinese and
English parsing because of substantial changes in the classification method,
and differences in treebank annotations.

As described in the previous section, the set of error categories considered
for Chinese is very different to the set of categories for English.  Even for
	some of the categories that were not substantially changed, errors may be
	classified differently because of cross-over between categories between two
	categories (\myeg between Verb taking wrong args and NP Attachment).

Differences in treebank annotations also present a challenge for cross-language
error comparison.  The most common error type in Chinese, NP-internal
structure, is rare in the English results, but
the datasets are not comparable because the \ptb has very limited NP-internal
structure annotated.  Further characterization of the impact of annotation
differences on errors is beyond the scope of this paper.

Three conclusions that can be made are that (i) coordination is a major issue in
both languages, (ii) PP attachment is a much greater problem in English, and
(iii) a higher frequency of trace-generating
syntax in Chinese compared to English poses substantial challenges.

\subsection{Cross-Parser Analysis} \label{sec:cross_parser_analysis}
The previous section described the error types and their distribution for a single Chinese parser.
Here we confirm that these are general trends, by showing that the same pattern
is observed for several different parsers on the \pctb 6 dev set.\footnote{
    We use the standard data split suggested by the \pctb 6 file manifest.
    As a result, our results differ from those previously reported on other splits.
		All analysis is on the dev set, to avoid revealing specific information
		about the test set.
}
We include results for
a transition-based parser \parencite[ZPAR;][]{Zhang-Clark:2009:ICPT},
a split-merge PCFG parser \parencite{Petrov-etal:2006,Petrov-Klein:2007,Petrov:2010:NAACLHLT},
a lexicalized parser \parencite{Bikel-Chiang:2000:CLP},
and a factored PCFG and dependency parser
\parencite{Levy-Manning:2003:ACL,Klein-Manning:2003:ACL,Klein-Manning:2003:NIPS}.
\footnote{These parsers represent a variety of parsing methods, though exclude
some recently developed parsers that are not publicly
available \parencite{Qian-Liu:2012:EMNLP,Xiong-etal:2005:IJCNLP}.}

Comparing the two Stanford parsers in Table~\ref{tab:comparison}, the factored
model provides clear improvements on sense disambiguation, but performs
slightly worse on coordination.

The Berkeley product parser we include uses only two grammars because we found,
in contrast to the English results \parencite{Petrov:2010:NAACLHLT}, that further
grammars provided limited benefits.  Comparing the performance with the
standard Berkeley parser it seems that the diversity in the grammars only
assists certain error types, with most of the improvement occurring in four of
the categories, while there is no improvement, or a slight decrease, in five categories.

\subsection{Tagging Error Impact} \label{sec:pos_ablation_study}

The challenge of accurate POS tagging in Chinese has been a major part of
several recent papers
\parencite{Qian-Liu:2012:EMNLP,Jiang-etal:2009:ACL,Forst-Fang:2009:EACL}.  The
Berk-G row of Table~\ref{tab:comparison} shows the performance of the Berkeley
parser when given gold POS tags.\footnote{We used the Berkeley parser as it was
the best of the parsers we considered.  Note that the Berkeley parser
occasionally prunes all of the parses that use the gold POS tags,
and so returns the best available alternative.  This leads to a POS accuracy of
99.35\%, which is still well above the parser's standard POS accuracy of
93.66\%.}
While the F$_1$ improvement is unsurprising, for the
first time we can clearly show that the gains are only in a subset of the error
types.  In particular, tagging improvement will not help for two of the most
significant challenges: coordination scope errors, and verb argument
selection.

To see which tagging confusions contribute to which error reductions, we adapt the POS
ablation approach of \textcite{Tse-Curran:2012:NAACL-HLT}.  We consider the POS
tag pairs shown in Table~\ref{tab:pos-confusion}.  To isolate the effects of
each confusion we start from the gold tags and introduce the output of the
Stanford tagger whenever it returns one of the two tags being
considered.\footnote{We introduce errors to gold tags, rather than removing
errors from automatic tags, isolating the effect of a single confusion
by eliminating interaction between tagging decisions.}
We then feed these ``semi-gold'' tags to the
Berkeley parser, and run the fine-grained error analysis on its output.

\begin{table}
  \centering
  \begin{tabular}{|llrr|}
    \hline
      \multicolumn{2}{c}{Confused tags} & Errors & $\Delta$ F$_1$ \\
    \hline
    \hline
      VV  & NN  & 1055 & -2.72 \\
      DEC & DEG &  526 & -1.72 \\
      JJ  & NN  &  297 & -0.57 \\
      NR  & NN  &  320 & -0.05 \\
    \hline
  \end{tabular}
  \caption{\label{tab:pos-confusion}
    The most frequently confused POS tag pairs.
    Each $\Delta$ F$_1$ is relative to Berk-G.
  }
\end{table}

\paragraph{VV/NN}  This confusion has been consistently shown to be a major
contributor to parsing errors
\parencite{Levy-Manning:2003:ACL,Tse-Curran:2012:NAACL-HLT,Qian-Liu:2012:EMNLP},
and we find a drop of over 2.7 $F_1$ when the output of the tagger is
introduced.  We found that while most error types have contributions from a
range of POS confusions, verb/noun confusion was responsible for virtually all of
the noun boundary errors corrected by using gold tags.

\paragraph{DEG/DEC}  This confusion between the relativizer and subordinator
senses of the particle \glos{的}{de} is the primary
source of improvements on modifier attachment when using gold tags.

\paragraph{NR/NN and JJ/NN}  Despite their frequency, these confusions have
little effect on parsing performance.  Even within the NP-internal error type
their impact is limited, and almost all of the errors do not change the
logical form.

\section{Summary}

The single F-score objective over brackets or dependencies obscures important differences between statistical parsers.
For instance, one or many mismatched brackets could be caused by a single attachment error.

We created a novel tree-transformation methodology for
evaluating parsers that categorises errors into linguistically meaningful
types.  Using this approach, we presented the first detailed examination of the
errors produced by a wide range of constituency parsers for
English and Chinese.  We found that PP attachment and clause attachment are the most
challenging constructions in English, while coordination turns out to be less problematic
than previously thought.  We also noted interesting variations in error types
for parsers variants.

We investigated the errors resolved in reranking, and introduced by changing
domains. We found that the Charniak rerankers improved most error types, but
made little headway on improving PP attachment.  Changing domain has an impact
on all error types, except NP internal structure.

We also quantified the relative impacts of a comprehensive set of error types
in Chinese parsing.  Our analysis has shown that while improvements in
Chinese POS tagging can make a substantial difference for some error types,
it will not address two high-frequency error types: incorrect verb argument
attachment and coordination scope.  The frequency of these two error types is
also unimproved by the use of products of latent variable grammars.  These
observations suggest that resolving the core challenges of Chinese parsing
will require new developments that suit the distinctive properties of Chinese
syntax.

We have released our system so that future constituent parsers can be evaluated
using our methodology.  Our analysis provides new insight into the development
of parsers over the past fifteen years, and the challenges that remain.

